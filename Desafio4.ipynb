{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPco9sMRf7zr8DTx06//n4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "518ecec770d2463894a73cc54fd40ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4399e29c7d141ebb40c4e05318b75b0",
              "IPY_MODEL_39ae058bada440479c7b8e13c470bbc9",
              "IPY_MODEL_2916e1eaab574a67af5fe664ffd9d5a5"
            ],
            "layout": "IPY_MODEL_a4b8b87a5904426b8e6265b7b10bd2d7"
          }
        },
        "d4399e29c7d141ebb40c4e05318b75b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "08/2025",
              "07/2025",
              "06/2025",
              "05/2025",
              "04/2025",
              "03/2025",
              "02/2025",
              "01/2025"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Competência:",
            "description_tooltip": null,
            "disabled": false,
            "index": 3,
            "layout": "IPY_MODEL_3dcf53e2826b44a680cac2c6a37cb34b",
            "style": "IPY_MODEL_6ba20889b8e54ae69a627fcf5baa9a8e"
          }
        },
        "39ae058bada440479c7b8e13c470bbc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "primary",
            "description": "Confirmar",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cf5e9e139fbd4baa8d1f93703cc496c1",
            "style": "IPY_MODEL_6b5bd495bdba44118144a0a28cf40d6c",
            "tooltip": ""
          }
        },
        "2916e1eaab574a67af5fe664ffd9d5a5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ac845e8ea9504b9fbbae258f5669895d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Competência selecionada: 05/2025\n",
                  "MES_REF=5, ANO_REF=2025, DIA_CORTE=15\n"
                ]
              }
            ]
          }
        },
        "a4b8b87a5904426b8e6265b7b10bd2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dcf53e2826b44a680cac2c6a37cb34b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba20889b8e54ae69a627fcf5baa9a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf5e9e139fbd4baa8d1f93703cc496c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5bd495bdba44118144a0a28cf40d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ac845e8ea9504b9fbbae258f5669895d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameliamacedogithub/repositorionaoseremoseliminados/blob/main/Desafio4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nome do grupo:** Não seremos eliminados\n",
        "\n",
        "Participantes:\n",
        "\n",
        "*   João Souza\n",
        "*   Maria Amélia França de Macedo\n",
        "*   Rodrigo Noll\n",
        "*   Sérgio Noll\n"
      ],
      "metadata": {
        "id": "PIxEp7v6ll0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline automatizado para cálculo de VR/VA\n",
        "- Leitura das bases\n",
        "- Padronização (nomes, datas, chaves)\n",
        "- EDA\n",
        "- Regras de exclusão\n",
        "- Cálculo de dias úteis elegíveis\n",
        "- Regra de desligamento (dia 15)\n",
        "- Cálculo de VR (80% empresa / 20% colaborador)\n",
        "- Geração do layout final (igual à \"VR MENSAL 05.2025\")\n"
      ],
      "metadata": {
        "id": "_ej5f_r8rzA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação de pré-requisitos\n"
      ],
      "metadata": {
        "id": "Lg_Syc_Xt_lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gdown\n",
        "!pip -q install pandas openpyxl xlsxwriter\n",
        "\n"
      ],
      "metadata": {
        "id": "Tjm-NAoDt6S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação de Bibliotecas"
      ],
      "metadata": {
        "id": "HPmwxEiar98Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os, re, zipfile, unicodedata,tempfile,gc\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import holidays\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TIsAiHFerxOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixar e extrair ZIP do Google Drive (Colab)\n",
        "- Usa gdown (lida com confirmações do Drive)\n",
        "- Extrai para uma pasta temporária\n",
        "- Lista o que foi extraído\n",
        "\n",
        "\n",
        "Se o link estiver quebrado irá solicitar usuário enviar arquivo zip os as planilhas, baixar e extrair ZIP (se for o caso)\n"
      ],
      "metadata": {
        "id": "uhfJjBmkoUbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_ID: Optional[str] = \"1SzLGxIRo2j8eHPdqe06xXxtoGGrWHoJ5\"  # ex: \"1SzLGxIRo2j8eHPdqe06xXxtoGGrWHoJ5\"\n",
        "DRIVE_URL: Optional[str] = 'https://drive.google.com/file/d/1SzLGxIRo2j8eHPdqe06xXxtoGGrWHoJ5/view?usp=drive_link'            # ex: \"https://drive.google.com/file/d/ABC123/view?usp=sharing\"\n",
        "\n",
        "# Pasta onde os arquivos ficarão disponíveis\n",
        "DEST_DIR = Path(\"/content/dados_vrva\")\n",
        "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Extensões consideradas úteis\n",
        "VALID_EXTS = {\".xlsx\", \".xls\", \".csv\"}\n",
        "\n",
        "# ====================== FUNÇÕES UTILITÁRIAS ======================\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def ensure_gdown():\n",
        "    try:\n",
        "        import gdown  # noqa: F401\n",
        "    except Exception:\n",
        "        import subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gdown\"])\n",
        "\n",
        "def extract_drive_id(url: str) -> Optional[str]:\n",
        "    pats = [\n",
        "        r\"/d/([a-zA-Z0-9_-]{20,})/\",\n",
        "        r\"id=([a-zA-Z0-9_-]{20,})\",\n",
        "        r\"/file/u/\\d+/d/([a-zA-Z0-9_-]{20,})/\",\n",
        "    ]\n",
        "    for p in pats:\n",
        "        m = re.search(p, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return None\n",
        "\n",
        "def download_zip_from_drive(file_id: Optional[str]=None, url: Optional[str]=None) -> Path:\n",
        "    \"\"\"Baixa um ZIP do Drive (via FILE_ID ou URL) para uma pasta temporária e retorna o path do arquivo ZIP.\"\"\"\n",
        "    ensure_gdown()\n",
        "    import gdown\n",
        "\n",
        "    if url and not file_id:\n",
        "        file_id = extract_drive_id(url)\n",
        "\n",
        "    if not file_id and not url:\n",
        "        raise ValueError(\"Nem FILE_ID nem DRIVE_URL foram informados.\")\n",
        "\n",
        "    tmp_dir = Path(tempfile.mkdtemp(prefix=\"vrva_zip_\"))\n",
        "    zip_path = tmp_dir / \"desafio_vrva.zip\"\n",
        "    print(f\"Baixando ZIP para: {zip_path}\")\n",
        "\n",
        "    if file_id:\n",
        "        gdown.download(id=file_id, output=str(zip_path), quiet=False)\n",
        "    else:\n",
        "        gdown.download(url=url, output=str(zip_path), quiet=False)\n",
        "\n",
        "    if not zip_path.exists() or zip_path.stat().st_size == 0:\n",
        "        raise RuntimeError(\"Download falhou ou retornou arquivo vazio.\")\n",
        "    return zip_path\n",
        "\n",
        "def safe_extract_zip(zip_path: Path, dest_dir: Path) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
        "    \"\"\"Extrai ZIP de forma tolerante. Retorna (ok, bad) com nomes e erros.\"\"\"\n",
        "    ok, bad = [], []\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        for info in z.infolist():\n",
        "            try:\n",
        "                z.extract(info, dest_dir)\n",
        "                ok.append(info.filename)\n",
        "            except Exception as e:\n",
        "                bad.append((info.filename, str(e)))\n",
        "    return ok, bad\n",
        "\n",
        "def inventory(dir_path: Path) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for p in dir_path.rglob(\"*\"):\n",
        "        if p.is_file():\n",
        "            rows.append({\n",
        "                \"relative_path\": str(p.relative_to(dir_path)),\n",
        "                \"size_kb\": round(p.stat().st_size/1024, 1),\n",
        "                \"ext\": p.suffix.lower()\n",
        "            })\n",
        "    if rows:\n",
        "        df = pd.DataFrame(rows).sort_values([\"ext\",\"relative_path\"]).reset_index(drop=True)\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=[\"relative_path\",\"size_kb\",\"ext\"])\n",
        "    return df\n",
        "\n",
        "def has_valid_files(dir_path: Path, valid_exts: set) -> bool:\n",
        "    return any(p.is_file() and p.suffix.lower() in valid_exts for p in dir_path.rglob(\"*\"))\n",
        "\n",
        "def prompt_upload_fallback(dest_dir: Path) -> None:\n",
        "    \"\"\"Solicita upload de ZIP ou planilhas individuais quando em Colab.\"\"\"\n",
        "    if not in_colab():\n",
        "        raise RuntimeError(\n",
        "            \"Falha no download/extração e este ambiente não é Google Colab. \"\n",
        "            \"Suba os arquivos manualmente para a pasta: {}\".format(dest_dir)\n",
        "        )\n",
        "    from google.colab import files  # type: ignore\n",
        "    print(\"\\n⚠️ Não foi possível obter planilhas válidas do Drive.\")\n",
        "    print(\"Por favor, faça upload de um arquivo .zip OU de um ou mais .xlsx/.csv.\")\n",
        "    uploaded = files.upload()  # dict {filename: bytes}\n",
        "\n",
        "    # Se for um único ZIP, extrai. Se forem planilhas soltas, apenas grava no destino.\n",
        "    if len(uploaded) == 1:\n",
        "        name, data = next(iter(uploaded.items()))\n",
        "        if name.lower().endswith(\".zip\"):\n",
        "            up_zip_path = dest_dir / name\n",
        "            with open(up_zip_path, \"wb\") as f:\n",
        "                f.write(data)\n",
        "            print(f\"Arquivo ZIP recebido: {up_zip_path}\")\n",
        "            ok, bad = safe_extract_zip(up_zip_path, dest_dir)\n",
        "            print(f\"✅ Extraídos: {len(ok)} | ⚠️ Erros: {len(bad)}\")\n",
        "            if bad:\n",
        "                for m, e in bad[:20]:\n",
        "                    print(f\" - {m} -> {e}\")\n",
        "        else:\n",
        "            # Não é zip: salva como planilha solta\n",
        "            out_path = dest_dir / name\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(data)\n",
        "            print(f\"Arquivo recebido: {out_path}\")\n",
        "    else:\n",
        "        # Múltiplos arquivos\n",
        "        for name, data in uploaded.items():\n",
        "            out_path = dest_dir / name\n",
        "            with open(out_path, \"wb\") as f:\n",
        "                f.write(data)\n",
        "        print(f\"{len(uploaded)} arquivos recebidos e gravados em: {dest_dir}\")\n",
        "\n",
        "# ====================== PIPELINE PRINCIPAL ======================\n",
        "def obter_planilhas(dest_dir: Path = DEST_DIR) -> Path:\n",
        "    \"\"\"\n",
        "    Tenta baixar e extrair um ZIP do Drive.\n",
        "    Se falhar (ou se não houver planilhas válidas), abre o fluxo de upload manual.\n",
        "    Retorna a pasta contendo os arquivos prontos para uso.\n",
        "    \"\"\"\n",
        "    # 1) Tenta via Drive (se configurado)\n",
        "    drive_attempted = False\n",
        "    if FILE_ID or DRIVE_URL:\n",
        "        drive_attempted = True\n",
        "        try:\n",
        "            zip_path = download_zip_from_drive(FILE_ID, DRIVE_URL)\n",
        "            print(f\"Pasta de extração: {dest_dir}\")\n",
        "            ok, bad = safe_extract_zip(zip_path, dest_dir)\n",
        "            print(f\"\\nArquivos extraídos com sucesso: {len(ok)}\")\n",
        "            print(f\"Arquivos com erro: {len(bad)}\")\n",
        "            if bad:\n",
        "                print(\"\\n⚠️ Houve entradas com erro na extração:\")\n",
        "                for m, e in bad[:20]:\n",
        "                    print(f\" - {m} -> {e}\")\n",
        "                if len(bad) > 20:\n",
        "                    print(f\"... e mais {len(bad)-20} itens\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Falha ao baixar/extrair do Drive: {e}\")\n",
        "\n",
        "    # 2) Verifica se há arquivos válidos; se não, pede upload\n",
        "    if not has_valid_files(dest_dir, VALID_EXTS):\n",
        "        if drive_attempted:\n",
        "            print(\"\\nNenhuma planilha válida encontrada após tentar o Drive.\")\n",
        "        try:\n",
        "            prompt_upload_fallback(dest_dir)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Falha no fallback de upload: {e}\")\n",
        "\n",
        "    # 3) Inventário final\n",
        "    df = inventory(dest_dir)\n",
        "    print(\"\\nInventário dos arquivos disponíveis:\")\n",
        "    try:\n",
        "        from IPython.display import display  # type: ignore\n",
        "        display(df)\n",
        "    except Exception:\n",
        "        print(df.to_string(index=False))\n",
        "\n",
        "    # 4) Checagem final\n",
        "    if not has_valid_files(dest_dir, VALID_EXTS):\n",
        "        raise RuntimeError(\"Nenhuma planilha .xlsx/.xls/.csv válida encontrada mesmo após upload.\")\n",
        "\n",
        "    print(f\"\\n✅ Arquivos prontos em: {dest_dir}\")\n",
        "    return dest_dir\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     obter_planilhas()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0iS-7CSVoU0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIA_CORTE = 15\n",
        "\n",
        "\n",
        "# mês corrente\n",
        "now = datetime.now(ZoneInfo(\"America/Sao_Paulo\"))\n",
        "ANO_ALVO = 2025\n",
        "limite = now.month if now.year == ANO_ALVO else 12\n",
        "meses = list(range(limite, 0, -1))\n",
        "\n",
        "opcoes = [f\"{m:02d}/{ANO_ALVO}\" for m in meses]\n",
        "dd = widgets.Dropdown(options=opcoes, value=opcoes[0], description=\"Competência:\")\n",
        "\n",
        "btn = widgets.Button(description=\"Confirmar\", button_style=\"primary\")\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_click(_):\n",
        "    global MES_REF, ANO_REF, DIA_CORTE   # <--- tornam-se globais\n",
        "    with out:\n",
        "        clear_output()\n",
        "        comp = dd.value\n",
        "        MES_REF, ANO_REF = comp.split(\"/\")\n",
        "        MES_REF, ANO_REF = int(MES_REF), int(ANO_REF)\n",
        "        print(f\"Competência selecionada: {comp}\")\n",
        "        print(f\"MES_REF={MES_REF}, ANO_REF={ANO_REF}, DIA_CORTE={DIA_CORTE}\")\n",
        "\n",
        "display(widgets.VBox([dd, btn, out]))\n",
        "btn.on_click(on_click)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "518ecec770d2463894a73cc54fd40ddc",
            "d4399e29c7d141ebb40c4e05318b75b0",
            "39ae058bada440479c7b8e13c470bbc9",
            "2916e1eaab574a67af5fe664ffd9d5a5",
            "a4b8b87a5904426b8e6265b7b10bd2d7",
            "3dcf53e2826b44a680cac2c6a37cb34b",
            "6ba20889b8e54ae69a627fcf5baa9a8e",
            "cf5e9e139fbd4baa8d1f93703cc496c1",
            "6b5bd495bdba44118144a0a28cf40d6c",
            "ac845e8ea9504b9fbbae258f5669895d"
          ]
        },
        "id": "zmX3q2wZEE2Z",
        "outputId": "a35d92c3-08c0-41aa-e07c-a3cadb29b5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='Competência:', options=('08/2025', '07/2025', '06/2025', '05/2025', '04/2…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "518ecec770d2463894a73cc54fd40ddc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caminhos de arquivos\n",
        "base_path = \"/content/dados_vrva\"\n",
        "adm_path = os.path.join(base_path, \"ADMISSÃO ABRIL.xlsx\")\n",
        "afa_path = os.path.join(base_path, \"AFASTAMENTOS.xlsx\")\n",
        "apr_path = os.path.join(base_path, \"APRENDIZ.xlsx\")\n",
        "ati_path = os.path.join(base_path, \"ATIVOS.xlsx\")\n",
        "dias_path = os.path.join(base_path, \"Base dias uteis.xlsx\")\n",
        "sind_path = os.path.join(base_path, \"Base sindicato x valor.xlsx\")\n",
        "des_path = os.path.join(base_path, \"DESLIGADOS.xlsx\")\n",
        "est_path = os.path.join(base_path, \"ESTÁGIO.xlsx\")\n",
        "ext_path = os.path.join(base_path, \"EXTERIOR.xlsx\")\n",
        "fer_path = os.path.join(base_path, \"FÉRIAS.xlsx\")\n",
        "vr_path  = os.path.join(base_path, \"VR MENSAL 05.2025.xlsx\")\n",
        "\n",
        "#TODO: checar se as planilhas carregadas correspondem a competência selecionada"
      ],
      "metadata": {
        "id": "9IOVtL5Rzw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- helpers ----------\n",
        "\n",
        "\n",
        "def strip_accents(s: str) -> str:\n",
        "    return ''.join(ch for ch in unicodedata.normalize('NFKD', str(s)) if not unicodedata.combining(ch))\n",
        "\n",
        "def norm_col(s: str) -> str:\n",
        "    \"\"\"Normaliza nome de coluna para matching: sem acento, minúscula, sem pontuação, espaço único.\"\"\"\n",
        "    s = strip_accents(str(s)).lower().strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    s = re.sub(r\"[^a-z0-9 ]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def looks_unnamed_token(x) -> bool:\n",
        "    sx = str(x) if x is not None else \"\"\n",
        "    sx_strip = sx.strip()\n",
        "    return (sx_strip == \"\") or norm_col(sx_strip).startswith(\"unnamed\")\n",
        "\n",
        "def is_numeric_like(x) -> bool:\n",
        "    sx = str(x).strip()\n",
        "    if sx == \"\" or sx.lower() == \"nan\":\n",
        "        return False\n",
        "    try:\n",
        "        float(sx.replace(\",\", \".\"))\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "#TODO: retirar o critério de keywords#\n",
        "\n",
        "def choose_header_row(df_nohdr: pd.DataFrame, max_scan: int = 15) -> int:\n",
        "    \"\"\"\n",
        "    Escolhe a melhor linha para virar cabeçalho.\n",
        "    Critérios (maior score é melhor):\n",
        "      +5 se contém keywords (matric, admiss, cargo, funcao)\n",
        "      +2 se contém letras (A–Z)\n",
        "      -1 se vazio/'unnamed'\n",
        "      -0.5 se numérico\n",
        "    Também penaliza linhas muito vazias.\n",
        "    \"\"\"\n",
        "    nrows = min(max_scan, len(df_nohdr))\n",
        "    best_idx, best_score = 0, -1e9\n",
        "    keywords = (\"matric\", \"admiss\", \"cargo\", \"funcao\", \"funçao\", \"função\", \"funca\")\n",
        "    for i in range(nrows):\n",
        "        row = df_nohdr.iloc[i]\n",
        "        score = 0.0\n",
        "        empties = 0\n",
        "        for v in row:\n",
        "            sv = str(v)\n",
        "            nk = norm_col(sv)\n",
        "            if nk == \"\" or nk.startswith(\"unnamed\"):\n",
        "                score -= 1.0\n",
        "                empties += 1\n",
        "            elif any(k in nk for k in keywords):\n",
        "                score += 5.0\n",
        "            elif re.search(r\"[a-zA-Z]\", sv):\n",
        "                score += 2.0\n",
        "            elif is_numeric_like(sv):\n",
        "                score -= 0.5\n",
        "        score -= empties * 0.2\n",
        "        if score > best_score:\n",
        "            best_score, best_idx = score, i\n",
        "    return best_idx\n",
        "\n",
        "def load_vr_with_header_detection(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lê a planilha VR e tenta detectar automaticamente a linha de cabeçalho.\n",
        "    Retorna um DataFrame com colunas definidas pela linha escolhida e dados a partir da próxima linha.\n",
        "    \"\"\"\n",
        "    # Lemos sem header para ter acesso a todas as linhas \"cruas\"\n",
        "    df_all = pd.read_excel(path, engine=\"openpyxl\", header=None)\n",
        "    if df_all.empty:\n",
        "        return df_all\n",
        "\n",
        "    hdr_idx = choose_header_row(df_all)\n",
        "    header_vals = list(df_all.iloc[hdr_idx].astype(str))\n",
        "\n",
        "    # Conserta nomes vazios/duplicados com rótulos \"col_i\"\n",
        "    fixed = []\n",
        "    seen = set()\n",
        "    for j, v in enumerate(header_vals):\n",
        "        name = v.strip()\n",
        "        if name == \"\" or looks_unnamed_token(name) or is_numeric_like(name):\n",
        "            name = f\"col_{j+1}\"\n",
        "        # garante unicidade\n",
        "        base, k, candidate = name, 1, name\n",
        "        while candidate in seen:\n",
        "            k += 1\n",
        "            candidate = f\"{base}_{k}\"\n",
        "        seen.add(candidate)\n",
        "        fixed.append(candidate)\n",
        "\n",
        "    df = df_all.iloc[hdr_idx + 1:].copy()\n",
        "    df.columns = fixed\n",
        "    # Remove colunas completamente vazias\n",
        "    df = df.loc[:, ~(df.isna().all(axis=0))]\n",
        "    df = df.reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "def looks_unnamed_header(c) -> bool:\n",
        "    cs = \"\" if c is None else str(c)\n",
        "    return (cs.strip() == \"\") or cs.startswith(\"Unnamed\")\n",
        "\n",
        "\n",
        "def find_col(df, *needles):\n",
        "    \"\"\"Encontra a primeira coluna de df cujo nome normalizado contenha qualquer 'needle'.\"\"\"\n",
        "    for c in df.columns:\n",
        "        nk = norm_col(c)\n",
        "        if any(n in nk for n in needles):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def dias_uteis_entre(inicio, fim):\n",
        "    \"\"\"Conta dias úteis de 'inicio' a 'fim' (inclusive). Se qualquer for NaT, retorna 0.\"\"\"\n",
        "    if pd.isna(inicio) or pd.isna(fim):\n",
        "        return 0\n",
        "    if fim < inicio:\n",
        "        return 0\n",
        "    return len(pd.bdate_range(inicio, fim))\n",
        "\n",
        "def proximo_corte(mes_ref: pd.Timestamp, dia_corte: int) -> pd.Timestamp:\n",
        "    \"\"\"Retorna a data de corte do mês seguinte ao MES_REF.\"\"\"\n",
        "    first_next_month = (MES_REF + pd.offsets.MonthBegin(1)).normalize()\n",
        "    # garante dia válido (se dia_corte > número de dias do mês, usa o último dia do mês)\n",
        "    last_day = (first_next_month + pd.offsets.MonthEnd(0)).day\n",
        "    day = min(DIA_CORTE, last_day)\n",
        "    return pd.Timestamp(first_next_month.year, first_next_month.month, day)\n",
        "\n",
        "def corte_atual(mes_ref: pd.Timestamp, dia_corte: int) -> pd.Timestamp:\n",
        "    \"\"\"Retorna a data de corte do MES_REF.\"\"\"\n",
        "    first_month = pd.Timestamp(ANO_REF, MES_REF, 1)\n",
        "    last_day = (first_month + pd.offsets.MonthEnd(0)).day\n",
        "    day = min(DIA_CORTE, last_day)\n",
        "    return pd.Timestamp(first_month.year, first_month.month, day)\n",
        "\n",
        "def dias_uteis_no_mes(mes_ref: pd.Timestamp) -> int:\n",
        "    \"\"\"Dias úteis (seg–sex) do mês completo de MES_REF.\"\"\"\n",
        "    inicio = pd.Timestamp(ANO_REF, MES_REF, 1)\n",
        "    fim    = inicio + pd.offsets.MonthEnd(0)\n",
        "    return len(pd.bdate_range(inicio, fim))\n",
        "\n",
        "def dias_uteis_ate_corte(mes_ref: pd.Timestamp, dia_corte: int) -> int:\n",
        "    \"\"\"Dias úteis do mês de MES_REF do dia 1 até DIA_CORTE (inclusivo).\"\"\"\n",
        "    inicio = pd.Timestamp(ANO_REF, MES_REF, 1)\n",
        "    ultimo_dia = (inicio + pd.offsets.MonthEnd(0)).day\n",
        "    dc = min(DIA_CORTE, ultimo_dia)\n",
        "    fim = pd.Timestamp(ANO_REF, MES_REF, dc)\n",
        "    return len(pd.bdate_range(inicio, fim))\n",
        "\n",
        "def calculo_dias(desconto: int, mes_ref: pd.Timestamp, dia_corte: int, base: str = \"mes\") -> int:\n",
        "    \"\"\"\n",
        "    Retorna o total de dias a pagar com base em dias úteis reais.\n",
        "    base=\"mes\"       -> usa todos os dias úteis do mês de MES_REF\n",
        "    base=\"ate_corte\" -> usa dias úteis do mês até o DIA_CORTE (inclusivo)\n",
        "    \"\"\"\n",
        "    if base == \"ate_corte\":\n",
        "        base_dias = dias_uteis_ate_corte(MES_REF, DIA_CORTE)\n",
        "    else:\n",
        "        base_dias = dias_uteis_no_mes(MES_REF)\n",
        "    return int(max(0, base_dias - int(desconto)))\n",
        "def feriados_br(years, estado=None, municipais=None):\n",
        "    \"\"\"\n",
        "    years: int ou lista de anos (ex.: 2025 ou [2024, 2025])\n",
        "    estado: UF (ex.: 'SP', 'RJ', 'GO') para incluir feriados estaduais\n",
        "    municipais: lista de datas municipais em 'YYYY-MM-DD' (strings) a excluir\n",
        "    \"\"\"\n",
        "    br = holidays.Brazil(years=years, subdiv=estado)  # nacionais + estaduais (se UF)\n",
        "    feriados = {pd.to_datetime(d).date() for d in br.keys()}\n",
        "    if municipais:\n",
        "        feriados |= {pd.to_datetime(d).date() for d in municipais}\n",
        "    return feriados\n",
        "\n",
        "def dias_uteis_periodo(inicio, fim, feriados):\n",
        "    \"\"\"Conta dias úteis (seg–sex) entre 'inicio' e 'fim' (inclusive), excluindo 'feriados'.\"\"\"\n",
        "    if pd.isna(inicio) or pd.isna(fim) or fim < inicio:\n",
        "        return 0\n",
        "    bd = pd.bdate_range(inicio, fim)\n",
        "    return int(sum(d.date() not in feriados for d in bd))\n",
        "\n",
        "\n",
        "# Liste aqui seus feriados municipais específicos (exemplos fictícios):\n",
        "FERIADOS_MUNICIPAIS = [\n",
        "    # \"2025-05-20\",  # exemplo: feriado municipal (substitua pelas datas reais)\n",
        "]\n",
        "\n",
        "UF='SP'\n",
        "\n",
        "feriados = feriados_br(years=ANO_REF, estado=UF, municipais=FERIADOS_MUNICIPAIS)\n",
        "\n",
        "# dias úteis do mês inteiro:\n",
        "inicio_mes = pd.Timestamp(ANO_REF, MES_REF, 1)\n",
        "fim_mes    = inicio_mes + pd.offsets.MonthEnd(0)\n",
        "dias_uteis_mes = dias_uteis_periodo(inicio_mes, fim_mes, feriados)\n",
        "\n",
        "# dias úteis até o corte (inclusivo):\n",
        "fim_corte = pd.Timestamp(ANO_REF, MES_REF, min(DIA_CORTE, (inicio_mes + pd.offsets.MonthEnd(0)).day))\n",
        "dias_uteis_ate_corte = dias_uteis_periodo(inicio_mes, fim_corte, feriados)\n",
        "\n",
        "dias_uteis_mes, dias_uteis_ate_corte\n",
        "\n",
        "df_vr  = load_vr_with_header_detection(vr_path)   # <<< usa detecção de cabeçalho\n"
      ],
      "metadata": {
        "id": "oMC8ouPZL5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratamento da planilha de admissão\n",
        "\n",
        "# ---------------- 1) Ler planilhas ----------------\n",
        "df_adm = pd.read_excel(adm_path, engine=\"openpyxl\")\n",
        "\n",
        "# ---------------- 2) ADMISSÃO: tratar 'obs' ----------------\n",
        "# Se houver coluna sem nome, renomeia a primeira para 'obs'\n",
        "new_cols, inserted = [], False\n",
        "for c in df_adm.columns:\n",
        "    if (not inserted) and looks_unnamed_header(c):\n",
        "        new_cols.append(\"obs\")\n",
        "        inserted = True\n",
        "    else:\n",
        "        new_cols.append(c)\n",
        "df_adm.columns = new_cols\n",
        "\n",
        "# Remove linhas onde 'obs' está preenchida e exclui a coluna\n",
        "if \"obs\" in df_adm.columns:\n",
        "    df_adm = df_adm[df_adm[\"obs\"].isna()].drop(columns=[\"obs\"])\n",
        "\n",
        "# ---------------- 3) VR: construir lookup de nomes EXATOS ----------------\n",
        "vr_cols = list(df_vr.columns)\n",
        "vr_lookup = {}\n",
        "for c in vr_cols:\n",
        "    key = norm_col(c)\n",
        "    if key:          # ignora vazios\n",
        "        vr_lookup.setdefault(key, c)  # mantém o primeiro se houver colisão\n",
        "\n",
        "# ---------------- 4) Renomear ADMISSÃO e descartar colunas sem equivalente ----------------\n",
        "mapping = {}\n",
        "for c in df_adm.columns:\n",
        "    key = norm_col(c)\n",
        "    target = vr_lookup.get(key)  # nome exato da VR\n",
        "    if target:\n",
        "        mapping[c] = target\n",
        "\n",
        "df_adm = df_adm.rename(columns=mapping)\n",
        "\n",
        "# Manter apenas colunas que existem na VR e reordenar na ordem da VR\n",
        "keep_cols = [c for c in df_vr.columns if c in df_adm.columns]\n",
        "df_adm = df_adm.loc[:, keep_cols]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wlRttrqdEmTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratamento da planilha Afastamentos\n",
        "\n",
        "\n",
        "\n",
        "# ---------------- 1) Ler planilhas ----------------\n",
        "df_afa = pd.read_excel(afa_path, engine=\"openpyxl\")\n",
        "\n",
        "# ---------------- 2) ADMISSÃO: tratar 'obs' ----------------\n",
        "# Se houver coluna sem nome, renomeia a primeira para 'obs'\n",
        "new_cols, inserted = [], False\n",
        "for c in df_afa.columns:\n",
        "    if (not inserted) and looks_unnamed_header(c):\n",
        "        new_cols.append(\"obs\")\n",
        "        inserted = True\n",
        "    else:\n",
        "        new_cols.append(c)\n",
        "df_afa.columns = new_cols\n",
        "\n",
        "\n",
        "# ---------------- 3) VR: construir lookup de nomes EXATOS ----------------\n",
        "vr_cols = list(df_vr.columns)\n",
        "vr_lookup = {}\n",
        "for c in vr_cols:\n",
        "    key = norm_col(c)\n",
        "    if key:          # ignora vazios\n",
        "        vr_lookup.setdefault(key, c)  # mantém o primeiro se houver colisão\n",
        "\n",
        "# ---------------- 4) Renomear AFASTAMENTO e manter SOMENTE 'Matricula' ----------------\n",
        "# Nome exato de 'Matricula' conforme VR\n",
        "mat_exato = next((c for c in df_vr.columns if norm_col(c) == \"matricula\"), \"Matricula\")\n",
        "\n",
        "# Renomeia colunas de AFASTAMENTO para os nomes exatos da VR quando houver equivalência\n",
        "mapping = {c: vr_lookup[norm_col(c)] for c in df_afa.columns if norm_col(c) in vr_lookup}\n",
        "df_afa = df_afa.rename(columns=mapping)\n",
        "\n",
        "# Garante que a coluna 'Matricula' tenha o nome exato da VR\n",
        "if mat_exato not in df_afa.columns:\n",
        "    cand = next((c for c in df_afa.columns if norm_col(c) == \"matricula\"), None)\n",
        "    if cand is not None:\n",
        "        df_afa = df_afa.rename(columns={cand: mat_exato})\n",
        "\n",
        "# Mantém somente a coluna 'Matricula' (exato conforme VR). Se não existir, cria DF vazio com essa coluna.\n",
        "if mat_exato in df_afa.columns:\n",
        "    df_afa = df_afa[[mat_exato]]\n",
        "else:\n",
        "    df_afa = pd.DataFrame(columns=[mat_exato])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wy1XmcMS1EOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tratamento da planilha Aprendiz,Estágio e Exterior\n",
        "\n",
        "#TODO para planilha Exterior - checar se a matrícula estiver na planilha ATIVOS,\n",
        "#e checar se está Trabalhando. Se estiver Trabalhando, paga, se não estiver exclui.\n",
        "\n",
        "# ---------------- 1) Ler planilhas ----------------\n",
        "\n",
        "df_apr = pd.read_excel(apr_path, engine=\"openpyxl\")\n",
        "df_est = pd.read_excel(est_path, engine=\"openpyxl\")\n",
        "df_ext = pd.read_excel(ext_path, engine=\"openpyxl\")\n",
        "\n",
        "\n",
        "# ---------------- 2) tratar 'obs' ----------------\n",
        "# Se houver coluna sem nome, renomeia a primeira para 'obs'\n",
        "new_cols, inserted = [], False\n",
        "for c in df_apr.columns:\n",
        "    if (not inserted) and looks_unnamed_header(c):\n",
        "        new_cols.append(\"obs\")\n",
        "        inserted = True\n",
        "    else:\n",
        "        new_cols.append(c)\n",
        "df_apr.columns = new_cols\n",
        "\n",
        "new_cols, inserted = [], False\n",
        "for c in df_est.columns:\n",
        "    if (not inserted) and looks_unnamed_header(c):\n",
        "        new_cols.append(\"obs\")\n",
        "        inserted = True\n",
        "    else:\n",
        "        new_cols.append(c)\n",
        "df_est.columns = new_cols\n",
        "\n",
        "new_cols, inserted = [], False\n",
        "for c in df_ext.columns:\n",
        "    if (not inserted) and looks_unnamed_header(c):\n",
        "        new_cols.append(\"obs\")\n",
        "        inserted = True\n",
        "    else:\n",
        "        new_cols.append(c)\n",
        "df_ext.columns = new_cols\n",
        "\n",
        "\n",
        "# ---------------- 3) VR: construir lookup de nomes EXATOS ----------------\n",
        "vr_cols = list(df_vr.columns)\n",
        "vr_lookup = {}\n",
        "for c in vr_cols:\n",
        "    key = norm_col(c)\n",
        "    if key:          # ignora vazios\n",
        "        vr_lookup.setdefault(key, c)  # mantém o primeiro se houver colisão\n",
        "\n",
        "# ---------------- 4) Renomear e manter SOMENTE 'Matricula' ----------------\n",
        "# Nome exato de 'Matricula' conforme VR\n",
        "mat_exato = next((c for c in df_vr.columns if norm_col(c) == \"matricula\"), \"Matricula\")\n",
        "\n",
        "# Renomeia colunas de APRENDIZ para os nomes exatos da VR quando houver equivalência\n",
        "mapping = {c: vr_lookup[norm_col(c)] for c in df_apr.columns if norm_col(c) in vr_lookup}\n",
        "df_apr = df_apr.rename(columns=mapping)\n",
        "\n",
        "# Renomeia colunas de ESTÁGIO para os nomes exatos da VR quando houver equivalência\n",
        "mapping = {c: vr_lookup[norm_col(c)] for c in df_est.columns if norm_col(c) in vr_lookup}\n",
        "df_est = df_est.rename(columns=mapping)\n",
        "\n",
        "\n",
        "# Garante que a coluna 'Matricula' tenha o nome exato da VR\n",
        "if mat_exato not in df_apr.columns:\n",
        "    cand = next((c for c in df_apr.columns if norm_col(c) == \"matricula\"), None)\n",
        "    if cand is not None:\n",
        "        df_apr = df_apr.rename(columns={cand: mat_exato})\n",
        "\n",
        "if mat_exato not in df_est.columns:\n",
        "    cand = next((c for c in df_est.columns if norm_col(c) == \"matricula\"), None)\n",
        "    if cand is not None:\n",
        "        df_est = df_est.rename(columns={cand: mat_exato})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Mantém somente a coluna 'Matricula' (exato conforme VR). Se não existir, cria DF vazio com essa coluna.\n",
        "if mat_exato in df_apr.columns:\n",
        "    df_apr = df_apr[[mat_exato]]\n",
        "else:\n",
        "    df_apr = pd.DataFrame(columns=[mat_exato])\n",
        "\n",
        "if mat_exato in df_est.columns:\n",
        "    df_est = df_est[[mat_exato]]\n",
        "else:\n",
        "    df_est = pd.DataFrame(columns=[mat_exato])\n",
        "\n",
        "# --- Renomear EXTERIOR e manter SOMENTE 'Matricula' (tratando 'cadastro' como sinônimo) ---\n",
        "\n",
        "# nome exato de 'Matricula' conforme VR\n",
        "mat_exato = next((c for c in df_vr.columns if norm_col(c) == \"matricula\"), \"Matricula\")\n",
        "\n",
        "# sinônimos que devemos tratar como 'matricula' no EXTERIOR\n",
        "SINONIMOS_MATRICULA = {\"matricula\", \"matric\", \"cadastro\", \"cad\", \"registro\", \"chapa\", \"idfunc\", \"idfuncionario\", \"funcional\"}\n",
        "\n",
        "# 1) mapear colunas do EXTERIOR para nomes exatos da VR\n",
        "mapping = {}\n",
        "for c in df_ext.columns:\n",
        "    nk = norm_col(c)\n",
        "    if nk in vr_lookup:\n",
        "        # nome já existe na VR -> renomeia para o exato\n",
        "        mapping[c] = vr_lookup[nk]\n",
        "    elif nk in SINONIMOS_MATRICULA:\n",
        "        # sinônimo de matrícula -> força renome para o nome exato da VR\n",
        "        mapping[c] = mat_exato\n",
        "\n",
        "df_ext = df_ext.rename(columns=mapping)\n",
        "\n",
        "# 2) garantir que a coluna 'Matricula' tenha o nome exato da VR\n",
        "if mat_exato not in df_ext.columns:\n",
        "    cand = next((c for c in df_ext.columns if norm_col(c) in SINONIMOS_MATRICULA), None)\n",
        "    if cand is not None:\n",
        "        df_ext = df_ext.rename(columns={cand: mat_exato})\n",
        "\n",
        "# 3) manter somente 'Matricula' (exato conforme VR). Se não existir, cria DF vazio com essa coluna.\n",
        "if mat_exato in df_ext.columns:\n",
        "    df_ext = df_ext[[mat_exato]]\n",
        "else:\n",
        "    df_ext = pd.DataFrame(columns=[mat_exato])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f4NqMOK2Mi2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================= Tratamento da planilha DESLIGADOS =======================\n",
        "import os, unicodedata, re\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/dados_vrva\"\n",
        "des_path = os.path.join(BASE_DIR, \"DESLIGADOS.xlsx\")\n",
        "\n",
        "# Helpers de normalização/localização de colunas\n",
        "def _strip_accents_lower(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    return re.sub(r\"\\s+\", \" \", s.strip()).lower()\n",
        "\n",
        "def _find_col(df: pd.DataFrame, keys) -> str | None:\n",
        "    \"\"\"Encontra a melhor coluna cujo nome contenha algum dos tokens de 'keys' (case/acentos-insensitive).\"\"\"\n",
        "    norm_cols = {c: _strip_accents_lower(c) for c in df.columns}\n",
        "    for c, n in norm_cols.items():\n",
        "        for k in keys:\n",
        "            if k in n:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def _data_corte(mes_ref: int, ano_ref: int, dia_corte: int) -> pd.Timestamp:\n",
        "    if mes_ref is None or ano_ref is None or dia_corte is None:\n",
        "        raise ValueError(\"Defina MES_REF, ANO_REF e DIA_CORTE antes de tratar DESLIGADOS.\")\n",
        "    base = pd.Timestamp(int(ano_ref), int(mes_ref), 1)\n",
        "    last_day = (base + pd.offsets.MonthEnd(0)).day\n",
        "    dia = min(int(dia_corte), int(last_day))\n",
        "    return pd.Timestamp(int(ano_ref), int(mes_ref), dia)\n",
        "\n",
        "def _dias_uteis_inclusivo(inicio: pd.Timestamp, fim: pd.Timestamp) -> int:\n",
        "    if pd.isna(inicio) or pd.isna(fim) or fim < inicio:\n",
        "        return 0\n",
        "    return len(pd.bdate_range(inicio, fim))  # dias ÚTEIS, inclusivo\n",
        "\n",
        "# 1) Ler DESLIGADOS\n",
        "df_des = pd.read_excel(des_path, engine=\"openpyxl\")\n",
        "\n",
        "# 2) Localizar colunas-alvo (tolerante a variações)\n",
        "col_mat = _find_col(df_des, [\"matricula\", \"nr matricula\", \"n matricula\", \"numero matricula\", \"registro\"])\n",
        "col_dt  = _find_col(df_des, [\"data demiss\", \"demissao\", \"demissão\", \"rescis\", \"saida\", \"saída\"])\n",
        "col_com = _find_col(df_des, [\"comunicado deslig\", \"comunic\", \"aviso\", \"desligamento\"])\n",
        "\n",
        "if not col_mat or not col_dt or not col_com:\n",
        "    raise KeyError(f\"Não achei todas as colunas necessárias em DESLIGADOS. \"\n",
        "                   f\"Encontradas: {list(df_des.columns)} | \"\n",
        "                   f\"matricula={col_mat}, data={col_dt}, comunicado={col_com}\")\n",
        "\n",
        "# 3) Normalizar tipos\n",
        "df_des[col_dt]  = pd.to_datetime(df_des[col_dt], errors=\"coerce\", dayfirst=True)\n",
        "df_des[col_com] = df_des[col_com].astype(str).str.strip().str.upper()\n",
        "\n",
        "# 4) Data de corte (DIA_CORTE/MES_REF/ANO_REF)\n",
        "DATA_CORTE = _data_corte(globals().get(\"MES_REF\"), globals().get(\"ANO_REF\"), globals().get(\"DIA_CORTE\"))\n",
        "\n",
        "# 5) Regras de exclusão e cálculo de Dias\n",
        "mask_excluir = (df_des[col_dt] < DATA_CORTE) & (df_des[col_com] == \"OK\")\n",
        "df_des = df_des.loc[~mask_excluir].copy()\n",
        "\n",
        "# Dias proporcionais para quem tem demissão >= data de corte\n",
        "df_des[\"Dias\"] = 0\n",
        "mask_calc = df_des[col_dt] >= DATA_CORTE\n",
        "df_des.loc[mask_calc, \"Dias\"] = df_des.loc[mask_calc, col_dt].apply(lambda d: _dias_uteis_inclusivo(DATA_CORTE, d)).astype(int)\n",
        "\n",
        "# 6) Alinhar nome exato da coluna Matricula com a planilha VR (se df_vr existir)\n",
        "mat_exato = \"Matricula\"\n",
        "if \"df_vr\" in globals():\n",
        "    mat_exato = next((c for c in df_vr.columns if _strip_accents_lower(c) == \"matricula\"), \"Matricula\")\n",
        "\n",
        "if col_mat != mat_exato:\n",
        "    df_des = df_des.rename(columns={col_mat: mat_exato})\n",
        "\n",
        "# 7) Manter somente Matricula (nome exato) e Dias\n",
        "df_des = df_des[[mat_exato, \"Dias\"]].reset_index(drop=True)\n",
        "\n",
        "# opcional: visualizar amostra\n",
        "# display(df_des.head())\n",
        "# ================================================================================\n",
        "\n",
        "\n",
        "# opcional: ver resultado\n",
        "df_des.head(100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MqA2KVQEPFdv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "9495b3ec-b806-47c6-88ec-5b7ad379e771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Matricula  Dias\n",
              "0       33492     0\n",
              "1       31394     2\n",
              "2       25670     1\n",
              "3       34387     1\n",
              "4       33711     2\n",
              "5       34819     2\n",
              "6       35474     2\n",
              "7       33835     2\n",
              "8       31880     9\n",
              "9       34534    11\n",
              "10      27226    11\n",
              "11      34477     0\n",
              "12      34884     2\n",
              "13      31314     0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88436653-fcf9-462a-abe0-9a522acc159b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Matricula</th>\n",
              "      <th>Dias</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33492</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31394</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25670</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34387</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33711</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>34819</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>35474</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>33835</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>31880</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>34534</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>27226</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>34477</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>34884</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>31314</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88436653-fcf9-462a-abe0-9a522acc159b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88436653-fcf9-462a-abe0-9a522acc159b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88436653-fcf9-462a-abe0-9a522acc159b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-77af71f8-f103-42ce-a991-e71c000049c8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77af71f8-f103-42ce-a991-e71c000049c8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-77af71f8-f103-42ce-a991-e71c000049c8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_des",
              "summary": "{\n  \"name\": \"df_des\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"Matricula\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2947,\n        \"min\": 25670,\n        \"max\": 35474,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          34534,\n          34477,\n          33492\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dias\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 11,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          11,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== Colab: Geração de Planilha VR =======================\n",
        "# Instala dependências (silencioso)\n",
        "!pip -q install pandas openpyxl xlsxwriter\n",
        "\n",
        "import os, re, unicodedata\n",
        "from typing import List, Optional\n",
        "import pandas as pd\n",
        "from xlsxwriter.utility import xl_rowcol_to_cell\n",
        "\n",
        "# --------------------------- Caminhos FIXOS no Colab ------------------------\n",
        "BASE_DIR = \"/content/dados_vrva\"\n",
        "PATH_ATIVOS = f\"{BASE_DIR}/ATIVOS.xlsx\"\n",
        "PATH_VR     = f\"{BASE_DIR}/VR MENSAL 05.2025.xlsx\"\n",
        "OUTPUT      = f\"{BASE_DIR}/NOVA_PLANILHA_VR.xlsx\"\n",
        "AUTO_DOWNLOAD = True  # baixar automaticamente ao final\n",
        "\n",
        "# Observação: MES_REF e ANO_REF devem existir em alguma célula anterior (ex.: MES_REF=5; ANO_REF=2025).\n",
        "# Se não existirem, o código tentará extrair do nome do arquivo de VR (padrão MM.AAAA).\n",
        "\n",
        "# -------------------- Utilidades de normalização e matching -----------------\n",
        "def _strip_accents_lower(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKD\", str(s)).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s.strip()).lower()\n",
        "    return s\n",
        "\n",
        "def _is_unnamed(val: Optional[str]) -> bool:\n",
        "    if val is None:\n",
        "        return True\n",
        "    s = str(val).strip()\n",
        "    if s == \"\" or s.lower() == \"nan\":\n",
        "        return True\n",
        "    return bool(re.match(r\"^unnamed[:\\s\\-]*\\d*\", s, flags=re.IGNORECASE))\n",
        "\n",
        "def _detect_header_row(df_raw: pd.DataFrame, scan_rows: int = 15) -> int:\n",
        "    scan_rows = min(scan_rows, len(df_raw))\n",
        "    best_idx, best_score = 0, -1\n",
        "    for i in range(scan_rows):\n",
        "        row = df_raw.iloc[i]\n",
        "        score = sum(1 for v in row if not _is_unnamed(v))\n",
        "        uniques = len(set([str(v).strip().lower() for v in row if not _is_unnamed(v)]))\n",
        "        dup_penalty = (score - uniques)\n",
        "        score = score - max(0, dup_penalty // 2)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_idx = i\n",
        "    return best_idx\n",
        "\n",
        "def _read_excel_flexible(path: str, sheet_hint: Optional[str] = None) -> pd.DataFrame:\n",
        "    try:\n",
        "        df_raw = pd.read_excel(path, sheet_name=sheet_hint if sheet_hint else 0, header=None, dtype=object)\n",
        "    except ValueError:\n",
        "        df_raw = pd.read_excel(path, sheet_name=0, header=None, dtype=object)\n",
        "\n",
        "    header_row = _detect_header_row(df_raw, scan_rows=15)\n",
        "    header_vals = [str(x).strip() if not pd.isna(x) else \"\" for x in df_raw.iloc[header_row].tolist()]\n",
        "\n",
        "    df = df_raw.iloc[header_row+1:].copy()\n",
        "    df.columns = header_vals\n",
        "\n",
        "    good_cols, seen = [], set()\n",
        "    for c in df.columns:\n",
        "        if _is_unnamed(c):\n",
        "            continue\n",
        "        cc = str(c).replace(\"\\n\", \" \").strip()\n",
        "        if cc == \"\":\n",
        "            continue\n",
        "        key = cc.lower()\n",
        "        if key in seen:\n",
        "            base, k = cc, 2\n",
        "            while f\"{base}.{k}\".lower() in seen:\n",
        "                k += 1\n",
        "            cc = f\"{base}.{k}\"\n",
        "            key = cc.lower()\n",
        "        seen.add(key)\n",
        "        good_cols.append(cc)\n",
        "    df = df.loc[:, good_cols]\n",
        "    return df\n",
        "\n",
        "def _best_match_column(target_name: str, candidates: List[str]) -> Optional[str]:\n",
        "    t_norm = _strip_accents_lower(target_name)\n",
        "    cand_norm = {c: _strip_accents_lower(c) for c in candidates}\n",
        "\n",
        "    # exato\n",
        "    for orig, norm in cand_norm.items():\n",
        "        if norm == t_norm:\n",
        "            return orig\n",
        "    # begins/contains\n",
        "    for orig, norm in cand_norm.items():\n",
        "        if norm.startswith(t_norm) or t_norm in norm:\n",
        "            return orig\n",
        "\n",
        "    # aliases\n",
        "    aliases = {\n",
        "        \"matricula\": [\"matricula\", \"matrícula\", \"nr matricula\", \"n matricula\", \"numero matricula\", \"registro\"],\n",
        "        \"sindicato do colaborador\": [\n",
        "            \"sindicato do colaborador\", \"sindicato colaborador\", \"sindicato (colaborador)\"\n",
        "        ],\n",
        "        \"sindicato\": [\"sindicato\", \"entidade sindical\"],\n",
        "        \"competência\": [\"competencia\", \"competência\", \"comp\", \"mes/ano\", \"mes-ano\"],\n",
        "        \"valor diário vr\": [\"valor diário vr\", \"valor diario vr\", \"valor vr dia\", \"vr diário\", \"vr dia\", \"valor dia vr\"],\n",
        "        \"total\": [\"total\", \"valor total\", \"vr total\"],\n",
        "        \"custo empresa\": [\"custo empresa\", \"custo da empresa\", \"custo p/ empresa\"],\n",
        "        \"desconto profissional\": [\"desconto profissional\", \"desconto do profissional\", \"desconto colab\"],\n",
        "        \"dias\": [\"dias\", \"qtd dias\", \"quantidade dias\"],\n",
        "        \"estado\": [\"estado\", \"uf\", \"unidade federativa\"],\n",
        "        \"valor\": [\"valor\", \"valor (r$)\", \"valor diario\"],\n",
        "        \"admissão\": [\"admissao\", \"admissão\", \"data de admissao\", \"dt admissao\", \"dt. admissao\"]\n",
        "    }\n",
        "    key = t_norm\n",
        "    if key in aliases:\n",
        "        for alias in aliases[key]:\n",
        "            a_norm = _strip_accents_lower(alias)\n",
        "            for orig, norm in cand_norm.items():\n",
        "                if norm == a_norm or norm.startswith(a_norm) or a_norm in norm:\n",
        "                    return orig\n",
        "\n",
        "    # fallback: maior interseção de tokens\n",
        "    t_tokens = set(t_norm.split())\n",
        "    best, best_overlap = None, 0\n",
        "    for orig, norm in cand_norm.items():\n",
        "        overlap = len(t_tokens.intersection(set(norm.split())))\n",
        "        if overlap > best_overlap:\n",
        "            best, best_overlap = orig, overlap\n",
        "    return best\n",
        "\n",
        "def _competencia_str(mes_ref: Optional[int], ano_ref: Optional[int], path_vr: Optional[str]) -> str:\n",
        "    if mes_ref is not None and ano_ref is not None:\n",
        "        return f\"{int(mes_ref):02d}/{int(ano_ref):04d}\"\n",
        "    if path_vr:\n",
        "        m = re.search(r\"(\\d{2})[.\\-_/ ]?(\\d{4})\", os.path.basename(path_vr))\n",
        "        if m:\n",
        "            mm, aaaa = m.group(1), m.group(2)\n",
        "            return f\"{mm}/{aaaa}\"\n",
        "    raise ValueError(\"Defina MES_REF e ANO_REF (ex.: MES_REF=5; ANO_REF=2025) ou renomeie o arquivo VR para conter MM.AAAA.\")\n",
        "\n",
        "def _get_matriculas_set(df: pd.DataFrame) -> set:\n",
        "    c = _best_match_column(\"MATRICULA\", list(df.columns))\n",
        "    if not c:\n",
        "        return set()\n",
        "    return set(df[c].dropna().astype(str).str.strip())\n",
        "\n",
        "# ----------------------------- Pipeline principal ---------------------------\n",
        "def gerar_planilha(path_ativos: str,\n",
        "                   path_vr: str,\n",
        "                   path_saida: str,\n",
        "                   competencia_str: str,\n",
        "                   sheet_ativos: Optional[str] = \"ATIVOS\",\n",
        "                   sheet_vr: Optional[str] = None) -> dict:\n",
        "\n",
        "    # 1) Lê ATIVOS e VR\n",
        "    df_ativos = _read_excel_flexible(path_ativos, sheet_hint=sheet_ativos)\n",
        "    df_vr = _read_excel_flexible(path_vr, sheet_hint=sheet_vr)\n",
        "    vr_cols = list(df_vr.columns)\n",
        "\n",
        "    # 1.1) Colunas essenciais em ATIVOS\n",
        "    c_atv_matricula = _best_match_column(\"MATRICULA\", list(df_ativos.columns))\n",
        "    c_atv_sindicato = _best_match_column(\"Sindicato\", list(df_ativos.columns))\n",
        "    if not c_atv_matricula:\n",
        "        raise KeyError(f\"Não encontrei a coluna de Matrícula no ATIVOS. Colunas: {list(df_ativos.columns)}\")\n",
        "\n",
        "    # 1.2) EXCLUSÕES de matrículas (df_afa, df_apr, df_des, df_est, df_ext, df_fer)\n",
        "    excl_names = [\"df_afa\",\"df_apr\",\"df_des\",\"df_est\",\"df_ext\",\"df_fer\"]\n",
        "    excl_set = set()\n",
        "    for nm in excl_names:\n",
        "        if nm in globals() and isinstance(globals()[nm], pd.DataFrame):\n",
        "            excl_set |= _get_matriculas_set(globals()[nm])\n",
        "\n",
        "    before_excl = len(df_ativos)\n",
        "    if excl_set:\n",
        "        df_ativos = df_ativos[~df_ativos[c_atv_matricula].astype(str).str.strip().isin(excl_set)].copy()\n",
        "    removed_count = before_excl - len(df_ativos)\n",
        "\n",
        "    # 1.3) INCLUSÕES de df_adm + captura de Admissão/Sindicato/Estado\n",
        "    adm_included = 0\n",
        "    adm_map = {}        # {matricula -> datetime(Admissão)}\n",
        "    adm_estado_map = {} # {matricula -> ESTADO (upper/normalized)}\n",
        "    adm_sind_map = {}   # {matricula -> sindicato (texto)}\n",
        "\n",
        "    if \"df_adm\" in globals() and isinstance(globals()[\"df_adm\"], pd.DataFrame):\n",
        "        df_adm = globals()[\"df_adm\"]\n",
        "        c_adm_matricula = _best_match_column(\"MATRICULA\", list(df_adm.columns))\n",
        "        c_adm_adm       = _best_match_column(\"Admissão\", list(df_adm.columns))\n",
        "        c_adm_sind      = _best_match_column(\"Sindicato\", list(df_adm.columns))\n",
        "        c_adm_estado    = _best_match_column(\"ESTADO\", list(df_adm.columns))\n",
        "\n",
        "        if c_adm_matricula:\n",
        "            mats_adm_all = df_adm[c_adm_matricula].dropna().astype(str).str.strip()\n",
        "            mats_ativos = set(df_ativos[c_atv_matricula].astype(str).str.strip())\n",
        "            mats_to_add = [m for m in mats_adm_all if m not in mats_ativos]\n",
        "\n",
        "            # dicionários auxiliares\n",
        "            if c_adm_adm:\n",
        "                for _, r in df_adm[[c_adm_matricula, c_adm_adm]].dropna(subset=[c_adm_matricula]).iterrows():\n",
        "                    m = str(r[c_adm_matricula]).strip()\n",
        "                    dt = pd.to_datetime(r[c_adm_adm], errors=\"coerce\")\n",
        "                    if pd.notna(dt):\n",
        "                        adm_map[m] = dt\n",
        "\n",
        "            if c_adm_sind:\n",
        "                for _, r in df_adm[[c_adm_matricula, c_adm_sind]].dropna(subset=[c_adm_matricula]).iterrows():\n",
        "                    adm_sind_map[str(r[c_adm_matricula]).strip()] = r[c_adm_sind]\n",
        "\n",
        "            if c_adm_estado:\n",
        "                def _norm_upper(s): return _strip_accents_lower(str(s)).upper()\n",
        "                for _, r in df_adm[[c_adm_matricula, c_adm_estado]].dropna(subset=[c_adm_matricula]).iterrows():\n",
        "                    adm_estado_map[str(r[c_adm_matricula]).strip()] = _norm_upper(r[c_adm_estado])\n",
        "\n",
        "            if mats_to_add:\n",
        "                # prepara df_extra com mesmas colunas de ATIVOS\n",
        "                df_extra = pd.DataFrame(columns=df_ativos.columns)\n",
        "                df_extra = df_extra.reindex(range(len(mats_to_add)))\n",
        "                df_extra[c_atv_matricula] = mats_to_add\n",
        "\n",
        "                # se ATIVOS tem coluna de sindicato e df_adm também, replica para ajudar o cálculo de VR\n",
        "                if c_atv_sindicato and c_atv_sindicato in df_extra.columns and c_adm_sind:\n",
        "                    df_extra[c_atv_sindicato] = [adm_sind_map.get(m) for m in mats_to_add]\n",
        "\n",
        "                # concatena\n",
        "                df_ativos = pd.concat([df_ativos, df_extra], ignore_index=True)\n",
        "                adm_included = len(mats_to_add)\n",
        "\n",
        "    # 2) Base nova com as mesmas colunas da VR (ATIVOS já filtrado/expandido)\n",
        "    df_new = pd.DataFrame(columns=vr_cols, index=range(len(df_ativos)))\n",
        "\n",
        "    # 3) Mapeamentos de colunas para a base final\n",
        "    c_new_matricula = _best_match_column(\"Matricula\", vr_cols)\n",
        "    c_new_sindcolab = _best_match_column(\"Sindicato do Colaborador\", vr_cols)\n",
        "    c_new_compet    = _best_match_column(\"Competência\", vr_cols)\n",
        "    c_new_admissao  = _best_match_column(\"Admissão\", vr_cols)\n",
        "\n",
        "    # 4) Copia dados-base\n",
        "    if c_atv_matricula and c_new_matricula:\n",
        "        df_new[c_new_matricula] = df_ativos[c_atv_matricula].values\n",
        "    if c_atv_sindicato and c_new_sindcolab:\n",
        "        df_new[c_new_sindcolab] = df_ativos[c_atv_sindicato].values\n",
        "\n",
        "    # 5) Competência\n",
        "    if c_new_compet and c_new_compet in df_new.columns:\n",
        "        df_new[c_new_compet] = competencia_str\n",
        "    else:\n",
        "        df_new[\"Competência\"] = competencia_str\n",
        "        c_new_compet = \"Competência\"\n",
        "\n",
        "    # 5.1) Admissão — padroniza como datetime sem hora (DD/MM/AAAA)\n",
        "    dest_adm = c_new_admissao if c_new_admissao in df_new.columns else \"Admissão\"\n",
        "    mats_series = df_new[c_new_matricula].astype(str).str.strip() if c_new_matricula in df_new.columns else pd.Series(index=df_new.index, dtype=\"object\")\n",
        "\n",
        "    # valores vindos do df_adm (se houver)\n",
        "    df_new[dest_adm] = mats_series.map(adm_map) if adm_map else pd.NaT\n",
        "\n",
        "    # Converte para datetime, entendendo dia primeiro (pt-BR) e removendo hora/timezone\n",
        "    df_new[dest_adm] = pd.to_datetime(df_new[dest_adm], errors=\"coerce\", dayfirst=True)\n",
        "    try:\n",
        "        df_new[dest_adm] = df_new[dest_adm].dt.tz_localize(None)\n",
        "    except Exception:\n",
        "        pass\n",
        "    df_new[dest_adm] = df_new[dest_adm].dt.normalize()  # zera hora para 00:00:00\n",
        "\n",
        "    # 6) Dias = 22 (default) + override pelas matrículas de df_des\n",
        "    c_new_dias = _best_match_column(\"Dias\", vr_cols)\n",
        "    destino_dias = c_new_dias if c_new_dias in df_new.columns else \"Dias\"\n",
        "    df_new[destino_dias] = 22  # default\n",
        "\n",
        "    # Se existir df_des com colunas Matricula e Dias, sobrescreve\n",
        "    if \"df_des\" in globals() and isinstance(df_des, pd.DataFrame) and not df_des.empty:\n",
        "        c_des_mat  = _best_match_column(\"Matricula\", list(df_des.columns))\n",
        "        c_des_dias = _best_match_column(\"Dias\",      list(df_des.columns))\n",
        "        if c_des_mat and c_des_dias and c_new_matricula in df_new.columns:\n",
        "            # mapa matricula -> dias (normalizando chaves e garantindo número)\n",
        "            des_map = (\n",
        "                df_des[[c_des_mat, c_des_dias]]\n",
        "                .dropna(subset=[c_des_mat])\n",
        "                .assign(_mat=lambda d: d[c_des_mat].astype(str).str.strip(),\n",
        "                        _dias=lambda d: d[c_des_dias].apply(lambda v: str(v).replace(\",\", \".\")))\n",
        "                .set_index(\"_mat\")[\"_dias\"]\n",
        "            )\n",
        "            des_map = pd.to_numeric(des_map, errors=\"coerce\")  # converte para número\n",
        "\n",
        "            # aplica às linhas correspondentes\n",
        "            keys = df_new[c_new_matricula].astype(str).str.strip()\n",
        "            override = keys.map(des_map)  # Series alinhada por índice de df_new\n",
        "            mask = override.notna()\n",
        "            df_new.loc[mask, destino_dias] = override[mask].astype(float)\n",
        "\n",
        "    # 7) VALOR DIÁRIO VR via planilha referência (Estado -> Valor) + DE-PARA\n",
        "    c_new_valor_dia = _best_match_column(\"VALOR DIÁRIO VR\", vr_cols)\n",
        "    destino_valor = c_new_valor_dia if c_new_valor_dia in df_new.columns else \"VALOR DIÁRIO VR\"\n",
        "\n",
        "    path_ref = os.path.join(BASE_DIR, \"Base sindicato x valor.xlsx\")\n",
        "    if not os.path.exists(path_ref):\n",
        "        raise FileNotFoundError(f\"Não achei a planilha de referência: {path_ref}\")\n",
        "    df_ref = _read_excel_flexible(path_ref, sheet_hint=None)\n",
        "\n",
        "    c_ref_estado = _best_match_column(\"ESTADO\", list(df_ref.columns))\n",
        "    c_ref_valor  = _best_match_column(\"VALOR\",  list(df_ref.columns))\n",
        "    if not c_ref_estado or not c_ref_valor:\n",
        "        raise KeyError(\n",
        "            \"Não encontrei as colunas 'ESTADO' e/ou 'VALOR' na planilha de referência. \"\n",
        "            f\"Colunas encontradas: {list(df_ref.columns)}\"\n",
        "        )\n",
        "\n",
        "    def _norm_upper(s): return _strip_accents_lower(s).upper()\n",
        "    ref_map = {}\n",
        "    for _, r in df_ref[[c_ref_estado, c_ref_valor]].dropna(subset=[c_ref_estado]).iterrows():\n",
        "        estado_norm = _norm_upper(str(r[c_ref_estado]))\n",
        "        try:\n",
        "            valor_num = float(str(r[c_ref_valor]).replace(\",\", \".\"))\n",
        "        except Exception:\n",
        "            valor_num = pd.to_numeric(r[c_ref_valor], errors=\"coerce\")\n",
        "            valor_num = float(valor_num) if pd.notna(valor_num) else 0.0\n",
        "        ref_map[estado_norm] = valor_num\n",
        "\n",
        "    sindicato_to_estado = {\n",
        "        \"SINDPD SP\": \"SÃO PAULO\",\n",
        "        \"SIND.TRAB.EM PROC DADOS E EMPR.EMPRESAS PROC DADOS ESTADO DE SP.\": \"SÃO PAULO\",\n",
        "        \"SINDPPD RS\": \"RIO GRANDE DO SUL\",\n",
        "        \"SINDICATO DOS TRAB. EM PROC. DE DADOS RIO GRANDE DO SUL\": \"RIO GRANDE DO SUL\",\n",
        "        \"SINDPD RJ\": \"RIO DE JANEIRO\",\n",
        "        \"SINDICATO PROFISSIONAIS DE PROC DADOS DO RIO DE JANEIRO\": \"RIO DE JANEIRO\",\n",
        "        \"SITEPD PR\": \"PARANÁ\",\n",
        "        \"SIND DOS TRAB EM EMPR PRIVADAS DE PROC DE DADOS DE CURITIBA E REGIAO METROPOLITANA\": \"PARANÁ\"\n",
        "    }\n",
        "\n",
        "    vr_val = pd.Series(0.0, index=df_new.index, dtype=\"float64\")\n",
        "    sind_series = df_new[c_new_sindcolab].astype(str).fillna(\"\") if c_new_sindcolab in df_new.columns else pd.Series(\"\", index=df_new.index)\n",
        "\n",
        "    for i in df_new.index:\n",
        "        s_norm = _norm_upper(sind_series.iloc[i]) if len(sind_series) else \"\"\n",
        "        estado = None\n",
        "        # 1) tenta mapear pelo Sindicato\n",
        "        for key, est in sindicato_to_estado.items():\n",
        "            if s_norm.startswith(_norm_upper(key)):\n",
        "                estado = est; break\n",
        "        if not estado:\n",
        "            for key, est in sindicato_to_estado.items():\n",
        "                if _norm_upper(key) in s_norm:\n",
        "                    estado = est; break\n",
        "        # 2) fallback: usa ESTADO do df_adm (se houver) para essa matrícula\n",
        "        if not estado and c_new_matricula in df_new.columns:\n",
        "            mat = str(df_new.at[i, c_new_matricula]).strip()\n",
        "            if mat in adm_estado_map:\n",
        "                estado = adm_estado_map[mat]\n",
        "\n",
        "        # aplica valor (0.0 se não encontrar)\n",
        "        if estado and _norm_upper(estado) in ref_map:\n",
        "            vr_val.at[i] = ref_map[_norm_upper(estado)]\n",
        "        else:\n",
        "            vr_val.at[i] = 0.0\n",
        "\n",
        "    df_new[destino_valor] = vr_val\n",
        "\n",
        "    # 8) TOTAL = Dias * VALOR DIÁRIO VR\n",
        "    c_new_total     = _best_match_column(\"TOTAL\", vr_cols)\n",
        "    destino_total = c_new_total if c_new_total in df_new.columns else \"TOTAL\"\n",
        "    dias_num   = pd.to_numeric(df_new[destino_dias], errors=\"coerce\").fillna(0)\n",
        "    valor_num  = pd.to_numeric(df_new[destino_valor], errors=\"coerce\").fillna(0.0)\n",
        "    df_new[destino_total] = (dias_num * valor_num).round(2)\n",
        "\n",
        "    # 9) Custo empresa = 80%\n",
        "    c_new_custo     = _best_match_column(\"Custo empresa\", vr_cols)\n",
        "    destino_custo = c_new_custo if c_new_custo in df_new.columns else \"Custo empresa\"\n",
        "    df_new[destino_custo] = (df_new[destino_total] * 0.80).round(2)\n",
        "\n",
        "    # 10) Desconto profissional = TOTAL - Custo empresa\n",
        "    c_new_desc      = _best_match_column(\"Desconto profissional\", vr_cols)\n",
        "    destino_desc  = c_new_desc if c_new_desc in df_new.columns else \"Desconto profissional\"\n",
        "    df_new[destino_desc] = (df_new[destino_total] - df_new[destino_custo]).round(2)\n",
        "\n",
        "    # 11) Exporta com cabeçalho na segunda linha + formatação + soma topo\n",
        "    with pd.ExcelWriter(path_saida, engine=\"xlsxwriter\") as writer:\n",
        "        # força dtype numérico nas colunas calculadas\n",
        "        for col_name in [destino_dias, destino_valor, destino_total, destino_custo, destino_desc]:\n",
        "            if col_name in df_new.columns:\n",
        "                df_new[col_name] = pd.to_numeric(df_new[col_name], errors=\"coerce\")\n",
        "\n",
        "        # garante datetime na coluna Admissão para formatação no Excel\n",
        "        if dest_adm in df_new.columns:\n",
        "            df_new[dest_adm] = pd.to_datetime(df_new[dest_adm], errors=\"coerce\")\n",
        "\n",
        "        df_new.to_excel(writer, index=False, sheet_name=\"BASE\", startrow=1)\n",
        "        ws = writer.sheets[\"BASE\"]\n",
        "\n",
        "        # cálculo automático\n",
        "        writer.book.set_calc_mode('automatic')\n",
        "        ws.freeze_panes(2, 0)\n",
        "\n",
        "        # formatos\n",
        "        num_fmt = writer.book.add_format({'num_format': '#,##0.00'})\n",
        "        date_fmt = writer.book.add_format({'num_format': 'dd/mm/yyyy'})\n",
        "\n",
        "        # aplica formato às colunas numéricas\n",
        "        for col_name in [destino_dias, destino_valor, destino_total, destino_custo, destino_desc]:\n",
        "            if col_name in df_new.columns:\n",
        "                col_idx = df_new.columns.get_loc(col_name)\n",
        "                ws.set_column(col_idx, col_idx, 15, num_fmt)\n",
        "\n",
        "        # aplica formato de data (DD/MM/AAAA) à coluna Admissão\n",
        "        if dest_adm in df_new.columns:\n",
        "            col_idx = df_new.columns.get_loc(dest_adm)\n",
        "            ws.set_column(col_idx, col_idx, 12, date_fmt)\n",
        "\n",
        "        # somatório no topo do TOTAL (linha 1)\n",
        "        if destino_total in df_new.columns:\n",
        "            total_col_idx = df_new.columns.get_loc(destino_total)\n",
        "            first_data_row = 3\n",
        "            last_data_row = len(df_new) + 2\n",
        "            top_cell = xl_rowcol_to_cell(0, total_col_idx)\n",
        "            range_str = f\"{xl_rowcol_to_cell(first_data_row-1, total_col_idx)}:{xl_rowcol_to_cell(last_data_row-1, total_col_idx)}\"\n",
        "            soma_val = float(df_new[destino_total].sum().round(2))\n",
        "            ws.write_formula(top_cell, f\"=SUM({range_str})\", num_fmt, soma_val)\n",
        "\n",
        "    return {\n",
        "        \"linhas_saida\": len(df_new),\n",
        "        \"colunas_saida\": list(df_new.columns),\n",
        "        \"arquivo\": path_saida,\n",
        "        \"mapeamentos\": {\n",
        "            \"Competência\": competencia_str,\n",
        "            \"Excluídas por status\": int(removed_count),\n",
        "            \"Incluídas de df_adm\": int(adm_included),\n",
        "            \"Coluna Admissão destino\": dest_adm,\n",
        "            \"Dias\": destino_dias,\n",
        "            \"VALOR DIÁRIO VR\": destino_valor,\n",
        "            \"TOTAL\": destino_total,\n",
        "            \"Custo empresa\": destino_custo,\n",
        "            \"Desconto profissional\": destino_desc,\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ------------------------------ Execução prática ----------------------------\n",
        "if not os.path.exists(PATH_ATIVOS):\n",
        "    raise FileNotFoundError(f\"Arquivo ATIVOS não encontrado em: {PATH_ATIVOS}\")\n",
        "if not os.path.exists(PATH_VR):\n",
        "    raise FileNotFoundError(f\"Arquivo de VR não encontrado em: {PATH_VR}\")\n",
        "\n",
        "_comp_str = _competencia_str(\n",
        "    mes_ref=globals().get(\"MES_REF\"),\n",
        "    ano_ref=globals().get(\"ANO_REF\"),\n",
        "    path_vr=PATH_VR\n",
        ")\n",
        "\n",
        "print(\">>> Arquivo ATIVOS:\", PATH_ATIVOS)\n",
        "print(\">>> Arquivo VR    :\", PATH_VR)\n",
        "print(\">>> Competência   :\", _comp_str)\n",
        "\n",
        "info = gerar_planilha(\n",
        "    PATH_ATIVOS,\n",
        "    PATH_VR,\n",
        "    OUTPUT,\n",
        "    competencia_str=_comp_str,\n",
        "    sheet_ativos=\"ATIVOS\",\n",
        "    sheet_vr=None\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Arquivo gerado:\")\n",
        "print(\"  - Caminho:\", info[\"arquivo\"])\n",
        "print(\"  - Linhas :\", info[\"linhas_saida\"])\n",
        "print(\"  - Colunas:\", len(info[\"colunas_saida\"]))\n",
        "print(\"  - Mapeamentos:\")\n",
        "for k, v in info[\"mapeamentos\"].items():\n",
        "    print(f\"    • {k}: {v}\")\n",
        "\n",
        "if AUTO_DOWNLOAD:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(OUTPUT)\n",
        "    except Exception as e:\n",
        "        print(\"[AVISO] Não foi possível iniciar o download automático:\", e)\n",
        "# ============================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "E10h26lh1Ri3",
        "outputId": "d5e3454d-0e1a-41eb-831c-26c71378b495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Arquivo ATIVOS: /content/dados_vrva/ATIVOS.xlsx\n",
            ">>> Arquivo VR    : /content/dados_vrva/VR MENSAL 05.2025.xlsx\n",
            ">>> Competência   : 05/2025\n",
            "\n",
            "✅ Arquivo gerado:\n",
            "  - Caminho: /content/dados_vrva/NOVA_PLANILHA_VR.xlsx\n",
            "  - Linhas : 1796\n",
            "  - Colunas: 10\n",
            "  - Mapeamentos:\n",
            "    • Competência: 05/2025\n",
            "    • Excluídas por status: 23\n",
            "    • Incluídas de df_adm: 4\n",
            "    • Coluna Admissão destino: Admissão\n",
            "    • Dias: Dias\n",
            "    • VALOR DIÁRIO VR: VALOR DIÁRIO VR\n",
            "    • TOTAL: TOTAL\n",
            "    • Custo empresa: Custo empresa\n",
            "    • Desconto profissional: Desconto profissional\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dabbcd28-657a-492c-9049-53a5e2a3c7f9\", \"NOVA_PLANILHA_VR.xlsx\", 60682)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Mediador CCT/ACT Scraper (MTE)\n",
        "--------------------------------\n",
        "Objetivo: Dado um CNPJ (e, opcionalmente, o nome do sindicato e uma data de referência),\n",
        "abrir o Sistema Mediador do MTE, consultar os instrumentos coletivos (CCT/ACT),\n",
        "listar resultados, selecionar o instrumento aplicável pela vigência e baixar o PDF.\n",
        "Depois, extrair regras de jornada/escala e banco de horas do PDF em texto estruturado.\n",
        "\n",
        "\n",
        "Requisitos (instale no ambiente antes de rodar):\n",
        "    pip install selenium undetected-chromedriver webdriver-manager pdfminer.six beautifulsoup4 pandas python-dateutil\n",
        "\n",
        "Uso básico (exemplo):\n",
        "    from mediador_cct_act_scraper import MediadorScraper\n",
        "\n",
        "    scraper = MediadorScraper(download_dir=\"./ccts\", headless=True)\n",
        "    resultados = scraper.buscar_por_cnpj(\"12.345.678/0001-90\")\n",
        "    # Opcional: ver todos os achados\n",
        "    for r in resultados:\n",
        "        print(r)\n",
        "\n",
        "    # Selecionar o instrumento aplicável para julho/2025, filtrando por sindicato (se quiser)\n",
        "    escolhido = MediadorScraper.selecionar_aplicavel(resultados, mes=7, ano=2025, sindicato_nome=\"SINDICATO X\")\n",
        "    if escolhido:\n",
        "        pdf_path = scraper.baixar_pdf(escolhido)\n",
        "        regras = scraper.extrair_regras_pdf(pdf_path)\n",
        "        print(regras)\n",
        "\n",
        "    scraper.fechar()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Optional, Dict, Any\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser as dateparser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "import undetected_chromedriver as uc\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "from pdfminer.high_level import extract_text\n",
        "\n",
        "# -------------------------------\n",
        "# Utilidades\n",
        "# -------------------------------\n",
        "\n",
        "def normalizar_cnpj(cnpj: str) -> str:\n",
        "    \"\"\"Remove caracteres não numéricos do CNPJ.\"\"\"\n",
        "    return re.sub(r\"\\D\", \"\", cnpj or \"\")\n",
        "\n",
        "\n",
        "def parse_data_br(texto: str) -> Optional[datetime]:\n",
        "    \"\"\"Tenta parsear uma data BR (dd/mm/aaaa).\"\"\"\n",
        "    if not texto:\n",
        "        return None\n",
        "    m = re.search(r\"(\\d{1,2})/(\\d{1,2})/(\\d{4})\", texto)\n",
        "    if not m:\n",
        "        return None\n",
        "    d, mth, y = map(int, m.groups())\n",
        "    try:\n",
        "        return datetime(y, mth, d)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Instrumento:\n",
        "    tipo: str  # CCT/ACT/TA\n",
        "    numero_registro: Optional[str]\n",
        "    numero_processo: Optional[str]\n",
        "    vigencia_inicio: Optional[datetime]\n",
        "    vigencia_fim: Optional[datetime]\n",
        "    sindicato_laboral: Optional[str]\n",
        "    sindicato_patronal: Optional[str]\n",
        "    cnpj_empresa: Optional[str]\n",
        "    municipio: Optional[str]\n",
        "    uf: Optional[str]\n",
        "    link_detalhe: Optional[str]\n",
        "    link_pdf: Optional[str] = None\n",
        "    titulo: Optional[str] = None\n",
        "\n",
        "    def cobre_mes_ano(self, mes: int, ano: int) -> bool:\n",
        "        if not self.vigencia_inicio or not self.vigencia_fim:\n",
        "            return False\n",
        "        base = datetime(ano, mes, 15)\n",
        "        return self.vigencia_inicio <= base <= self.vigencia_fim\n",
        "\n",
        "\n",
        "class MediadorScraper:\n",
        "    BASE_URL = \"https://www3.mte.gov.br/sistemas/mediador/\"\n",
        "    CONSULTA_URL = BASE_URL + \"consultarinstcoletivo\"\n",
        "\n",
        "    def __init__(self, download_dir: str = \"./downloads_mediador\", headless: bool = True, timeout: int = 25):\n",
        "        self.download_dir = os.path.abspath(download_dir)\n",
        "        os.makedirs(self.download_dir, exist_ok=True)\n",
        "        self.timeout = timeout\n",
        "        self.driver = self._init_driver(headless=headless)\n",
        "\n",
        "    def _init_driver(self, headless: bool = True):\n",
        "        opts = uc.ChromeOptions()\n",
        "        if headless:\n",
        "            opts.add_argument(\"--headless=new\")\n",
        "        opts.add_argument(\"--no-sandbox\")\n",
        "        opts.add_argument(\"--disable-dev-shm-usage\")\n",
        "        prefs = {\n",
        "            \"download.default_directory\": self.download_dir,\n",
        "            \"download.prompt_for_download\": False,\n",
        "            \"download.directory_upgrade\": True,\n",
        "            \"safebrowsing.enabled\": True,\n",
        "        }\n",
        "        opts.add_experimental_option(\"prefs\", prefs)\n",
        "        driver = uc.Chrome(options=opts)\n",
        "        driver.set_page_load_timeout(self.timeout)\n",
        "        return driver\n",
        "\n",
        "    def fechar(self):\n",
        "        try:\n",
        "            self.driver.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # -------------------------------\n",
        "    # Consulta por CNPJ\n",
        "    # -------------------------------\n",
        "    def buscar_por_cnpj(self, cnpj: str) -> List[Instrumento]:\n",
        "        cnpj_digits = normalizar_cnpj(cnpj)\n",
        "        if len(cnpj_digits) != 14:\n",
        "            raise ValueError(\"CNPJ inválido. Informe 14 dígitos (com ou sem pontuação).\")\n",
        "\n",
        "        self.driver.get(self.CONSULTA_URL)\n",
        "        # Espera por possível captcha\n",
        "        self._aguardar_captcha(max_espera=60)\n",
        "\n",
        "        # Tenta localizar o campo CNPJ de forma robusta (por label, por placeholder, por proximidade de texto)\n",
        "        campo_cnpj = self._encontrar_input_relacionado_a_texto([\"CNPJ\", \"CNPJ do participante\", \"CNPJ:\"])\n",
        "        if not campo_cnpj:\n",
        "            raise RuntimeError(\"Não foi possível localizar o campo CNPJ na página de consulta.\")\n",
        "        campo_cnpj.clear()\n",
        "        campo_cnpj.send_keys(cnpj_digits)\n",
        "\n",
        "        # Tenta clicar no botão de pesquisar\n",
        "        botao = self._encontrar_botao_por_texto([\"Pesquisar\", \"Consultar\", \"Buscar\", \"Localizar\"])\n",
        "        if not botao:\n",
        "            raise RuntimeError(\"Não foi possível localizar o botão de pesquisa na página de consulta.\")\n",
        "        botao.click()\n",
        "\n",
        "        # Aguarda resultados renderizarem\n",
        "        time.sleep(2)\n",
        "        self._aguardar_resultados()\n",
        "\n",
        "        html = self.driver.page_source\n",
        "        return self._parse_resultados(html, cnpj_digits)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Consulta por sindicato (fallback)\n",
        "    # -------------------------------\n",
        "    def buscar_por_sindicato(self, nome: str, uf: Optional[str] = None, municipio: Optional[str] = None) -> List[Instrumento]:\n",
        "        self.driver.get(self.CONSULTA_URL)\n",
        "        self._aguardar_captcha(max_espera=60)\n",
        "\n",
        "        # Campo \"Participante\" / \"Sindicatos\" costuma existir\n",
        "        campo_participante = self._encontrar_input_relacionado_a_texto([\"Participante\", \"Sindic\", \"Entidade\", \"Razão Social\"])\n",
        "        if not campo_participante:\n",
        "            raise RuntimeError(\"Não foi possível localizar o campo de participante/sindicato.\")\n",
        "        campo_participante.clear()\n",
        "        campo_participante.send_keys(nome)\n",
        "\n",
        "        # UF/município são opcionais\n",
        "        if uf:\n",
        "            campo_uf = self._encontrar_input_relacionado_a_texto([\"UF\", \"Estado\"])\n",
        "            if campo_uf:\n",
        "                try:\n",
        "                    campo_uf.clear(); campo_uf.send_keys(uf)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        if municipio:\n",
        "            campo_mun = self._encontrar_input_relacionado_a_texto([\"Município\", \"Municipio\"])\n",
        "            if campo_mun:\n",
        "                try:\n",
        "                    campo_mun.clear(); campo_mun.send_keys(municipio)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        botao = self._encontrar_botao_por_texto([\"Pesquisar\", \"Consultar\", \"Buscar\", \"Localizar\"])\n",
        "        if not botao:\n",
        "            raise RuntimeError(\"Não foi possível localizar o botão de pesquisa na página de consulta.\")\n",
        "        botao.click()\n",
        "\n",
        "        time.sleep(2)\n",
        "        self._aguardar_resultados()\n",
        "        html = self.driver.page_source\n",
        "        return self._parse_resultados(html, None)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Parsing de resultados\n",
        "    # -------------------------------\n",
        "    def _parse_resultados(self, html: str, cnpj_digits: Optional[str]) -> List[Instrumento]:\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        instrumentos = []\n",
        "\n",
        "        # Procura a primeira tabela \"grande\"\n",
        "        tabelas = soup.find_all(\"table\")\n",
        "        for tab in tabelas:\n",
        "            linhas = tab.find_all(\"tr\")\n",
        "            # Heurística: tabela de resultados costuma ter cabeçalho com \"Vigência\" ou \"Registro\"\n",
        "            header = [th.get_text(strip=True) for th in tab.find_all(\"th\")]\n",
        "            if not header:\n",
        "                # Algumas versões usam <td> como cabeçalho\n",
        "                header = [td.get_text(strip=True) for td in linhas[0].find_all(\"td\")] if linhas else []\n",
        "            cabecalho_ok = any(\"Vig\" in h or \"Registro\" in h or \"Instrumento\" in h for h in header)\n",
        "            if not cabecalho_ok:\n",
        "                continue\n",
        "\n",
        "            # Processa linhas\n",
        "            for tr in linhas[1:]:\n",
        "                cols = [c.get_text(\" \", strip=True) for c in tr.find_all([\"td\", \"th\"])]\n",
        "                if len(cols) < 3:\n",
        "                    continue\n",
        "                link_det = tr.find(\"a\")\n",
        "                href = link_det.get(\"href\") if link_det else None\n",
        "                # Extrai campos por heurística\n",
        "                tipo = \"\"\n",
        "                numero_registro = None\n",
        "                numero_processo = None\n",
        "                vig_ini = vig_fim = None\n",
        "                sindicato_l = sindicato_p = None\n",
        "                municipio = uf = None\n",
        "                titulo = None\n",
        "\n",
        "                row_text = \" \".join(cols)\n",
        "                titulo = row_text[:180]\n",
        "\n",
        "                # Tipo do instrumento (CCT/ACT/TA)\n",
        "                m_tipo = re.search(r\"\\b(Conven\\w+ Coletiv\\w+|Acordo Coletiv\\w+|Termo Aditiv\\w+)\\b\", row_text, re.I)\n",
        "                if m_tipo:\n",
        "                    palavra = m_tipo.group(1).lower()\n",
        "                    if \"acordo\" in palavra:\n",
        "                        tipo = \"ACT\"\n",
        "                    elif \"termo\" in palavra:\n",
        "                        tipo = \"TA\"\n",
        "                    else:\n",
        "                        tipo = \"CCT\"\n",
        "\n",
        "                # Número de registro\n",
        "                m_reg = re.search(r\"Registro\\s*(?:no\\s*MTE|MTE)?\\s*[:\\-]?\\s*([A-Z]{2}\\d{6}/\\d{4}|\\w{2,}\\d{2,}/\\d{4})\", row_text, re.I)\n",
        "                if m_reg:\n",
        "                    numero_registro = m_reg.group(1)\n",
        "\n",
        "                # Número do processo\n",
        "                m_proc = re.search(r\"Processo\\s*[:\\-]?\\s*([\\d\\./\\-]+)\", row_text, re.I)\n",
        "                if m_proc:\n",
        "                    numero_processo = m_proc.group(1)\n",
        "\n",
        "                # Vigência (dd/mm/aaaa a dd/mm/aaaa)\n",
        "                m_vig = re.search(r\"(\\d{2}/\\d{2}/\\d{4}).*?(\\d{2}/\\d{2}/\\d{4})\", row_text)\n",
        "                if m_vig:\n",
        "                    vig_ini = parse_data_br(m_vig.group(1))\n",
        "                    vig_fim = parse_data_br(m_vig.group(2))\n",
        "\n",
        "                # Sindicatos\n",
        "                m_sind_l = re.search(r\"Sindic\\w+\\s+Laboral\\s*[:\\-]?\\s*(.+?)\\s{2,}\", row_text, re.I)\n",
        "                m_sind_p = re.search(r\"Sindic\\w+\\s+Patronal\\s*[:\\-]?\\s*(.+?)\\s{2,}\", row_text, re.I)\n",
        "                if m_sind_l:\n",
        "                    sindicato_l = m_sind_l.group(1).strip()\n",
        "                if m_sind_p:\n",
        "                    sindicato_p = m_sind_p.group(1).strip()\n",
        "\n",
        "                # UF/município\n",
        "                m_uf = re.search(r\"\\b(UF|Estado)[:\\-]?\\s*([A-Z]{2})\\b\", row_text)\n",
        "                if m_uf:\n",
        "                    uf = m_uf.group(2)\n",
        "                m_mun = re.search(r\"Munic[ií]pio[:\\-]?\\s*([A-Za-z \\-']{3,})\\b\", row_text)\n",
        "                if m_mun:\n",
        "                    municipio = m_mun.group(1).strip()\n",
        "\n",
        "                instrumentos.append(Instrumento(\n",
        "                    tipo=tipo or \"\",\n",
        "                    numero_registro=numero_registro,\n",
        "                    numero_processo=numero_processo,\n",
        "                    vigencia_inicio=vig_ini,\n",
        "                    vigencia_fim=vig_fim,\n",
        "                    sindicato_laboral=sindicato_l,\n",
        "                    sindicato_patronal=sindicato_p,\n",
        "                    cnpj_empresa=cnpj_digits,\n",
        "                    municipio=municipio,\n",
        "                    uf=uf,\n",
        "                    link_detalhe=(href if href and href.startswith(\"http\") else (self.BASE_URL + href.lstrip(\"/\")) if href else None),\n",
        "                    titulo=titulo,\n",
        "                ))\n",
        "\n",
        "        # Dedup básica por (registro, link)\n",
        "        uniq = {}\n",
        "        for it in instrumentos:\n",
        "            k = (it.numero_registro, it.link_detalhe)\n",
        "            if k not in uniq:\n",
        "                uniq[k] = it\n",
        "        return list(uniq.values())\n",
        "\n",
        "    def _aguardar_resultados(self):\n",
        "        # Qualquer tabela ou elemento com \"Resultados\"/\"Instrumento\"/\"Vigência\"\n",
        "        try:\n",
        "            WebDriverWait(self.driver, self.timeout).until(\n",
        "                EC.presence_of_element_located((By.XPATH, \"//*[contains(translate(text(),'VIG','vig'),'vig')]\"))\n",
        "            )\n",
        "        except TimeoutException:\n",
        "            pass\n",
        "\n",
        "    # -------------------------------\n",
        "    # Seleção do instrumento aplicável\n",
        "    # -------------------------------\n",
        "    @staticmethod\n",
        "    def selecionar_aplicavel(resultados: List[Instrumento], mes: int, ano: int, sindicato_nome: Optional[str] = None, preferir_tipo: Optional[str] = None) -> Optional[Instrumento]:\n",
        "        \"\"\"Escolhe o instrumento cuja vigência cobre (mes/ano). Se sindicato_nome for dado,\n",
        "        filtra pelo nome aproximado. preferir_tipo pode ser \"CCT\" ou \"ACT\".\"\"\"\n",
        "        def _norm(s):\n",
        "            return re.sub(r\"[^a-z0-9]\", \"\", s.lower()) if s else \"\"\n",
        "\n",
        "        alvo = datetime(ano, mes, 15)\n",
        "        cand = []\n",
        "        for it in resultados:\n",
        "            if it.vigencia_inicio and it.vigencia_fim and it.vigencia_inicio <= alvo <= it.vigencia_fim:\n",
        "                if sindicato_nome:\n",
        "                    n = _norm(sindicato_nome)\n",
        "                    if n not in _norm(it.sindicato_laboral) and n not in _norm(it.titulo or \"\"):\n",
        "                        continue\n",
        "                cand.append(it)\n",
        "\n",
        "        if not cand:\n",
        "            return None\n",
        "\n",
        "        # Preferir tipo, depois mais recente pela data de início\n",
        "        if preferir_tipo:\n",
        "            preferir_tipo = preferir_tipo.upper()\n",
        "            cand_tipo = [c for c in cand if c.tipo.upper() == preferir_tipo]\n",
        "            if cand_tipo:\n",
        "                cand = cand_tipo\n",
        "\n",
        "        cand.sort(key=lambda x: (x.vigencia_inicio or datetime.min), reverse=True)\n",
        "        return cand[0]\n",
        "\n",
        "    # -------------------------------\n",
        "    # Baixar PDF\n",
        "    # -------------------------------\n",
        "    def baixar_pdf(self, instrumento: Instrumento) -> str:\n",
        "        \"\"\"Abre a página de detalhe e tenta achar um link de PDF para baixar.\"\"\"\n",
        "        if not instrumento.link_detalhe:\n",
        "            raise ValueError(\"Instrumento não possui link de detalhe.\")\n",
        "\n",
        "        self.driver.get(instrumento.link_detalhe)\n",
        "        self._aguardar_captcha(max_espera=60)\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Procura links para PDF\n",
        "        links = self.driver.find_elements(By.XPATH, \"//a[contains(@href,'.pdf') or contains(translate(text(),'PDF','pdf'),'pdf')]\")\n",
        "        href = None\n",
        "        for a in links:\n",
        "            try:\n",
        "                h = a.get_attribute(\"href\")\n",
        "                if h and h.lower().endswith(\".pdf\"):\n",
        "                    href = h\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        if not href:\n",
        "            # Tenta capturar PDF via botão de imprimir/visualizar\n",
        "            possiveis = self.driver.find_elements(By.XPATH, \"//a | //button\")\n",
        "            for el in possiveis:\n",
        "                try:\n",
        "                    txt = (el.text or \"\").strip().lower()\n",
        "                    if any(p in txt for p in [\"pdf\", \"imprimir\", \"visualizar\", \"ver inteiro\", \"abrir\"]):\n",
        "                        el.click()\n",
        "                        time.sleep(2)\n",
        "                        # após click, checar novamente se há <a href=...pdf>\n",
        "                        links2 = self.driver.find_elements(By.XPATH, \"//a[contains(@href,'.pdf')]\")\n",
        "                        if links2:\n",
        "                            href = links2[0].get_attribute(\"href\")\n",
        "                            break\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if not href:\n",
        "            raise RuntimeError(\"Não foi possível localizar o link do PDF no detalhe do instrumento.\")\n",
        "\n",
        "        # Download manual (usando requests pode falhar por sessão; então usar o próprio navegador)\n",
        "        # Vamos abrir o PDF em nova aba e salvar via streaming do Chrome (diretório de downloads)\n",
        "        self.driver.execute_script(\"window.open(arguments[0], '_blank');\", href)\n",
        "        time.sleep(3)\n",
        "\n",
        "        # Como alternativa simples, baixar via urllib com cookies\n",
        "        pdf_name = self._nome_pdf(instrumento, href)\n",
        "        pdf_path = os.path.join(self.download_dir, pdf_name)\n",
        "\n",
        "        # Fallback: tentar baixar com requests + cookies do Selenium\n",
        "        try:\n",
        "            import requests\n",
        "            s = requests.Session()\n",
        "            for c in self.driver.get_cookies():\n",
        "                s.cookies.set(c['name'], c['value'], domain=c.get('domain'))\n",
        "            r = s.get(href, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            with open(pdf_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "        except Exception:\n",
        "            # Último recurso: o arquivo pode estar na pasta de download do Chrome headless\n",
        "            # Tentativa de achar algum .pdf recém criado lá\n",
        "            time.sleep(5)\n",
        "            candidatos = [f for f in os.listdir(self.download_dir) if f.lower().endswith('.pdf')]\n",
        "            if candidatos:\n",
        "                # pega o mais recente\n",
        "                cand = max([os.path.join(self.download_dir, f) for f in candidatos], key=os.path.getmtime)\n",
        "                shutil.move(cand, pdf_path)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        instrumento.link_pdf = href\n",
        "        return pdf_path\n",
        "\n",
        "    def _nome_pdf(self, it: Instrumento, href: str) -> str:\n",
        "        base = it.numero_registro or it.titulo or os.path.basename(href).split('?')[0]\n",
        "        base = re.sub(r\"\\W+\", \"_\", base)[:80]\n",
        "        return f\"{base}.pdf\"\n",
        "\n",
        "    # -------------------------------\n",
        "    # Extração de regras do PDF\n",
        "    # -------------------------------\n",
        "    @staticmethod\n",
        "    def extrair_regras_pdf(pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extrai texto do PDF e busca padrões comuns de jornada/escala/banco de horas.\n",
        "        Retorna um dicionário com achados e trechos relevantes.\n",
        "        \"\"\"\n",
        "        texto = extract_text(pdf_path) or \"\"\n",
        "        texto_norm = re.sub(r\"\\s+\", \" \", texto)\n",
        "\n",
        "        # Heurísticas de extração\n",
        "        regras: Dict[str, Any] = {\n",
        "            \"jornada_semanais_horas\": None,\n",
        "            \"jornada_mensais_horas\": None,\n",
        "            \"escala\": None,\n",
        "            \"banco_de_horas\": None,\n",
        "            \"intervalo_intrajornada\": None,\n",
        "            \"repouso_semanal\": None,\n",
        "            \"horas_extras\": None,\n",
        "            \"trechos_relevantes\": {}\n",
        "        }\n",
        "\n",
        "        # Jornada semanal/mensal\n",
        "        m_sem = re.search(r\"(\\b3[0-9]|4[0-8])\\s*horas\\s*semanais\", texto_norm, re.I)\n",
        "        if m_sem:\n",
        "            regras[\"jornada_semanais_horas\"] = int(re.sub(\"\\D\", \"\", m_sem.group(0))[:2])\n",
        "            regras[\"trechos_relevantes\"][\"jornada_semanais\"] = m_sem.group(0)\n",
        "        m_mens = re.search(r\"(1\\d\\d|2\\d\\d)\\s*horas\\s*mensais\", texto_norm, re.I)\n",
        "        if m_mens:\n",
        "            regras[\"jornada_mensais_horas\"] = int(re.sub(\"\\D\", \"\", m_mens.group(0))[:3])\n",
        "            regras[\"trechos_relevantes\"][\"jornada_mensais\"] = m_mens.group(0)\n",
        "\n",
        "        # Escalas típicas\n",
        "        m_esc = re.search(r\"\\b(12\\s*[xX]\\s*36|6\\s*[xX]\\s*1|5\\s*[xX]\\s*2|24\\s*[xX]\\s*72|4\\s*[xX]\\s*2)\\b\", texto_norm)\n",
        "        if m_esc:\n",
        "            regras[\"escala\"] = re.sub(r\"\\s+\", \"\", m_esc.group(1))\n",
        "            regras[\"trechos_relevantes\"][\"escala\"] = m_esc.group(0)\n",
        "\n",
        "        # Banco de horas / compensação\n",
        "        m_bh = re.search(r\"(banco\\s+de\\s+horas|compensa[cç][aã]o\\s+de\\s+jornada).*?(\\.|;)\", texto_norm, re.I)\n",
        "        if m_bh:\n",
        "            regras[\"banco_de_horas\"] = m_bh.group(0)\n",
        "            regras[\"trechos_relevantes\"][\"banco_de_horas\"] = m_bh.group(0)\n",
        "\n",
        "        # Intervalo intrajornada\n",
        "        m_int = re.search(r\"intervalo\\s+intrajornada.*?(\\.|;)\", texto_norm, re.I)\n",
        "        if m_int:\n",
        "            regras[\"intervalo_intrajornada\"] = m_int.group(0)\n",
        "            regras[\"trechos_relevantes\"][\"intervalo_intrajornada\"] = m_int.group(0)\n",
        "\n",
        "        # Repouso semanal\n",
        "        m_rsr = re.search(r\"repouso\\s+semanal\\s+remunerado.*?(\\.|;)\", texto_norm, re.I)\n",
        "        if m_rsr:\n",
        "            regras[\"repouso_semanal\"] = m_rsr.group(0)\n",
        "            regras[\"trechos_relevantes\"][\"rsr\"] = m_rsr.group(0)\n",
        "\n",
        "        # Horas extras\n",
        "        m_he = re.search(r\"hora[s]?\\s+extra[s]?.*?(\\.|;)\", texto_norm, re.I)\n",
        "        if m_he:\n",
        "            regras[\"horas_extras\"] = m_he.group(0)\n",
        "            regras[\"trechos_relevantes\"][\"horas_extras\"] = m_he.group(0)\n",
        "\n",
        "        regras[\"_debug_tamanho_texto\"] = len(texto_norm)\n",
        "        return regras\n",
        "\n",
        "    # -------------------------------\n",
        "    # Helpers de página\n",
        "    # -------------------------------\n",
        "    def _aguardar_captcha(self, max_espera: int = 60):\n",
        "        \"\"\"Se aparecer \"captcha\" no HTML, aguarda até sumir (ou até max_espera).\"\"\"\n",
        "        t0 = time.time()\n",
        "        while time.time() - t0 < max_espera:\n",
        "            html = (self.driver.page_source or \"\").lower()\n",
        "            if \"captcha\" in html and \"cloudflare\" not in html:\n",
        "                print(\"[Aviso] Captcha detectado: resolva manualmente na aba do navegador...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                return\n",
        "\n",
        "    def _encontrar_input_relacionado_a_texto(self, textos: List[str]):\n",
        "        # Busca por label + input\n",
        "        for t in textos:\n",
        "            try:\n",
        "                el = self.driver.find_element(By.XPATH, f\"//label[contains(translate(normalize-space(.),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), '{t.lower()}')]/following::input[1]\")\n",
        "                if el:\n",
        "                    return el\n",
        "            except Exception:\n",
        "                pass\n",
        "        # Busca por texto solto seguido de input\n",
        "        for t in textos:\n",
        "            try:\n",
        "                el = self.driver.find_element(By.XPATH, f\"//*[contains(translate(normalize-space(text()),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), '{t.lower()}')]/following::input[1]\")\n",
        "                if el:\n",
        "                    return el\n",
        "            except Exception:\n",
        "                pass\n",
        "        # Fallback: primeiro input da página\n",
        "        try:\n",
        "            return self.driver.find_element(By.XPATH, \"//input[@type='text' or @type='search']\")\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def _encontrar_botao_por_texto(self, textos: List[str]):\n",
        "        for t in textos:\n",
        "            try:\n",
        "                el = self.driver.find_element(By.XPATH, f\"//button[contains(translate(normalize-space(.),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), '{t.lower()}')]\")\n",
        "                if el: return el\n",
        "            except Exception:\n",
        "                pass\n",
        "        for t in textos:\n",
        "            try:\n",
        "                el = self.driver.find_element(By.XPATH, f\"//input[@type='submit' and contains(translate(@value,'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), '{t.lower()}')]\")\n",
        "                if el: return el\n",
        "            except Exception:\n",
        "                pass\n",
        "        for t in textos:\n",
        "            try:\n",
        "                el = self.driver.find_element(By.XPATH, f\"//a[contains(translate(normalize-space(.),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), '{t.lower()}')]\")\n",
        "                if el: return el\n",
        "            except Exception:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Execução por CLI (opcional)\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    ap = argparse.ArgumentParser(description=\"Consulta CCT/ACT no Sistema Mediador (MTE) e extrai regras de jornada/escala.\")\n",
        "    ap.add_argument(\"--cnpj\", help=\"CNPJ da empresa (com ou sem pontuação)\")\n",
        "    ap.add_argument(\"--mes\", type=int, default=None, help=\"Mês de referência (1-12)\")\n",
        "    ap.add_argument(\"--ano\", type=int, default=None, help=\"Ano de referência (ex.: 2025)\")\n",
        "    ap.add_argument(\"--sindicato\", type=str, default=None, help=\"Nome (ou parte) do sindicato laboral\")\n",
        "    ap.add_argument(\"--preferir\", type=str, default=None, choices=[\"CCT\",\"ACT\"], help=\"Preferir CCT ou ACT, se ambos cobrirem a vigência\")\n",
        "    ap.add_argument(\"--download_dir\", type=str, default=\"./downloads_mediador\", help=\"Pasta para salvar PDFs\")\n",
        "    ap.add_argument(\"--headless\", action=\"store_true\", help=\"Rodar navegador em modo headless\")\n",
        "\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    if not args.cnpj:\n",
        "        ap.error(\"Informe --cnpj\")\n",
        "\n",
        "    scraper = MediadorScraper(download_dir=args.download_dir, headless=args.headless)\n",
        "    try:\n",
        "        resultados = scraper.buscar_por_cnpj(args.cnpj)\n",
        "        if not resultados:\n",
        "            print(\"Nenhum instrumento encontrado para o CNPJ informado.\")\n",
        "            raise SystemExit(0)\n",
        "\n",
        "        if args.mes and args.ano:\n",
        "            escolhido = MediadorScraper.selecionar_aplicavel(resultados, mes=args.mes, ano=args.ano, sindicato_nome=args.sindicato, preferir_tipo=args.preferir)\n",
        "        else:\n",
        "            # pega o mais recente\n",
        "            escolhido = sorted(resultados, key=lambda x: (x.vigencia_inicio or datetime.min), reverse=True)[0]\n",
        "\n",
        "        if not escolhido:\n",
        "            print(\"Não foi possível selecionar um instrumento aplicável com os filtros fornecidos.\")\n",
        "            raise SystemExit(0)\n",
        "\n",
        "        print(\"Selecionado:\\n\", json.dumps({k:v for k,v in asdict(escolhido).items() if k != 'link_pdf'}, default=str, ensure_ascii=False, indent=2))\n",
        "\n",
        "        pdf_path = scraper.baixar_pdf(escolhido)\n",
        "        print(f\"PDF salvo em: {pdf_path}\")\n",
        "\n",
        "        regras = scraper.extrair_regras_pdf(pdf_path)\n",
        "        print(\"Regras extraídas:\\n\", json.dumps(regras, ensure_ascii=False, indent=2))\n",
        "\n",
        "    finally:\n",
        "        scraper.fechar()\n"
      ],
      "metadata": {
        "id": "wcm4klm2qtJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}